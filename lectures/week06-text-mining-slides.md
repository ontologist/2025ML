# Week 6: Text Mining & Feature Extraction
# ç¬¬6é€±ï¼šãƒ†ã‚­ã‚¹ãƒˆãƒã‚¤ãƒ‹ãƒ³ã‚°ã¨ç‰¹å¾´æŠ½å‡º

**Course:** Machine Learning and Intelligence | æ©Ÿæ¢°å­¦ç¿’ã¨çŸ¥èƒ½
**Instructor:** Yuri Tijerino
**Duration:** 15-20 minutes

---

## Slide 1: Welcome to Week 6!
## ã‚¹ãƒ©ã‚¤ãƒ‰1ï¼šç¬¬6é€±ã¸ã‚ˆã†ã“ãï¼

**Topic:** Text Mining & Feature Extraction
**ãƒˆãƒ”ãƒƒã‚¯:** ãƒ†ã‚­ã‚¹ãƒˆãƒã‚¤ãƒ‹ãƒ³ã‚°ã¨ç‰¹å¾´æŠ½å‡º

**Today's Objectives | ä»Šæ—¥ã®ç›®æ¨™:**
- Understand why computers can't read text directly | ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãŒãƒ†ã‚­ã‚¹ãƒˆã‚’ç›´æ¥èª­ã‚ãªã„ç†ç”±ã‚’ç†è§£ã™ã‚‹
- Learn text preprocessing techniques | ãƒ†ã‚­ã‚¹ãƒˆå‰å‡¦ç†æŠ€è¡“ã‚’å­¦ã¶
- Master Bag of Words and TF-IDF | Bag of Wordsã¨TF-IDFã‚’ç¿’å¾—ã™ã‚‹
- Handle Japanese text processing | æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†ã‚’æ‰±ã†
- Prepare company descriptions for ML | MLç”¨ã«ä¼æ¥­èª¬æ˜ã‚’æº–å‚™ã™ã‚‹

---

## Slide 2: The Text Problem
## ã‚¹ãƒ©ã‚¤ãƒ‰2:ãƒ†ã‚­ã‚¹ãƒˆã®å•é¡Œ

### Computers Don't Understand Text
### ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã¯ãƒ†ã‚­ã‚¹ãƒˆã‚’ç†è§£ã—ãªã„

**The Challenge | èª²é¡Œ:**
- Machine learning algorithms need NUMBERS | æ©Ÿæ¢°å­¦ç¿’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¯æ•°å€¤ãŒå¿…è¦
- Company descriptions are TEXT | ä¼æ¥­èª¬æ˜ã¯ãƒ†ã‚­ã‚¹ãƒˆ
- We need to convert text â†’ numbers | ãƒ†ã‚­ã‚¹ãƒˆâ†’æ•°å€¤ã¸ã®å¤‰æ›ãŒå¿…è¦

**Example | ä¾‹:**
```
âŒ Computer sees: "Leading AI company"
   ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãŒè¦‹ã‚‹ã‚‚ã®ï¼šã€ŒLeading AI companyã€

âœ… Computer needs: [0.4, 0.6, 0.3, 0.5, ...]
   ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãŒå¿…è¦ã¨ã™ã‚‹ã‚‚ã®ï¼š[0.4, 0.6, 0.3, 0.5, ...]
```

**Solution | è§£æ±ºç­–:**
**Text Mining** = Extracting numerical features from text
**ãƒ†ã‚­ã‚¹ãƒˆãƒã‚¤ãƒ‹ãƒ³ã‚°** = ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æ•°å€¤ç‰¹å¾´ã‚’æŠ½å‡ºã™ã‚‹ã“ã¨

---

## Slide 3: Text Mining Pipeline
## ã‚¹ãƒ©ã‚¤ãƒ‰3ï¼šãƒ†ã‚­ã‚¹ãƒˆãƒã‚¤ãƒ‹ãƒ³ã‚°ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

### Five Essential Steps | 5ã¤ã®å¿…é ˆã‚¹ãƒ†ãƒƒãƒ—

```
1. CLEAN TEXT        â†’    2. TOKENIZE       â†’    3. NORMALIZE
   ãƒ†ã‚­ã‚¹ãƒˆã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°          ãƒˆãƒ¼ã‚¯ãƒ³åŒ–              æ­£è¦åŒ–
   Remove noise              Split into words        Lowercase, stopwords

   â†“

4. STEM/LEMMATIZE   â†’    5. VECTORIZE
   èªå¹¹æŠ½å‡º/ãƒ¬ãƒ³ãƒåŒ–            ãƒ™ã‚¯ãƒˆãƒ«åŒ–
   Reduce to roots           Convert to numbers
```

**Example Through Pipeline | ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’é€šã˜ãŸä¾‹:**

```
Original: "We're providing AI solutions for businesses!"
   â†“ Clean
"We are providing AI solutions for businesses"
   â†“ Tokenize
["We", "are", "providing", "AI", "solutions", "for", "businesses"]
   â†“ Normalize
["providing", "ai", "solutions", "businesses"]
   â†“ Vectorize
[0, 1, 0, 1, 0, 1, 1, 0, ...]
```

---

## Slide 4: Step 1 - Text Cleaning
## ã‚¹ãƒ©ã‚¤ãƒ‰4ï¼šã‚¹ãƒ†ãƒƒãƒ—1 - ãƒ†ã‚­ã‚¹ãƒˆã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°

### Removing Noise | ãƒã‚¤ã‚ºã®é™¤å»

**What to Remove | å‰Šé™¤ã™ã‚‹ã‚‚ã®:**
- ğŸ—‘ï¸ HTML tags: `<p>`, `<div>`, `<br>`
- ğŸ—‘ï¸ Special characters: @, #, $, %, &
- ğŸ—‘ï¸ Extra whitespace: multiple spaces, tabs
- ğŸ—‘ï¸ URLs: http://example.com
- ğŸ—‘ï¸ Numbers (sometimes): depends on context

**Example | ä¾‹:**
```
Before: "<p>Leading tech company! ğŸš€ Visit: www.example.com</p>"
å‰ï¼šã€Œ<p>Leading tech company! ğŸš€ Visit: www.example.com</p>ã€

After: "Leading tech company"
å¾Œï¼šã€ŒLeading tech companyã€
```

**Why | ãªãœ:**
Noise doesn't help classification, just creates useless features
ãƒã‚¤ã‚ºã¯åˆ†é¡ã«å½¹ç«‹ãŸãšã€ç„¡é§„ãªç‰¹å¾´ã‚’ä½œã‚‹ã ã‘

---

## Slide 5: Step 2 - Tokenization
## ã‚¹ãƒ©ã‚¤ãƒ‰5ï¼šã‚¹ãƒ†ãƒƒãƒ—2 - ãƒˆãƒ¼ã‚¯ãƒ³åŒ–

### Breaking Text into Words | ãƒ†ã‚­ã‚¹ãƒˆã‚’å˜èªã«åˆ†å‰²

**English Tokenization | è‹±èªã®ãƒˆãƒ¼ã‚¯ãƒ³åŒ–:**
```
Text: "Machine learning is amazing"
ãƒ†ã‚­ã‚¹ãƒˆï¼šã€ŒMachine learning is amazingã€

Tokens: ["Machine", "learning", "is", "amazing"]
ãƒˆãƒ¼ã‚¯ãƒ³ï¼š["Machine", "learning", "is", "amazing"]
```
âœ… Easy: Split by spaces | ç°¡å˜ï¼šã‚¹ãƒšãƒ¼ã‚¹ã§åˆ†å‰²

**Japanese Tokenization | æ—¥æœ¬èªã®ãƒˆãƒ¼ã‚¯ãƒ³åŒ–:**
```
Text: "æ©Ÿæ¢°å­¦ç¿’ã¯ç´ æ™´ã‚‰ã—ã„"
ãƒ†ã‚­ã‚¹ãƒˆï¼šã€Œæ©Ÿæ¢°å­¦ç¿’ã¯ç´ æ™´ã‚‰ã—ã„ã€

Wrong âŒ: ["æ©Ÿ", "æ¢°", "å­¦", "ç¿’", "ã¯", "ç´ ", "æ™´", "ã‚‰", "ã—", "ã„"]
é–“é•ã„ï¼šå˜ä¸€æ–‡å­—ã«åˆ†å‰²

Correct âœ…: ["æ©Ÿæ¢°å­¦ç¿’", "ã¯", "ç´ æ™´ã‚‰ã—ã„"]
æ­£ã—ã„ï¼šæ„å‘³ã®ã‚ã‚‹å˜ä½ã«åˆ†å‰²
```

**Japanese Tools | æ—¥æœ¬èªãƒ„ãƒ¼ãƒ«:**
- **MeCab**: Fast, accurate, commonly used
- **Janome**: Pure Python, easier installation
- **SudachiPy**: Handles compound words well

---

## Slide 6: Step 3 - Normalization
## ã‚¹ãƒ©ã‚¤ãƒ‰6ï¼šã‚¹ãƒ†ãƒƒãƒ—3 - æ­£è¦åŒ–

### Making Text Consistent | ãƒ†ã‚­ã‚¹ãƒˆã‚’ä¸€è²«ã•ã›ã‚‹

**Normalization Techniques | æ­£è¦åŒ–æŠ€è¡“:**

1. **Lowercase | å°æ–‡å­—åŒ–**
   ```
   "Technology" â†’ "technology"
   "TECHNOLOGY" â†’ "technology"
   ```

2. **Remove Stopwords | ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰é™¤å»**
   ```
   English: "the", "is", "are", "and", "of"
   Japanese: "ã¯", "ãŒ", "ã‚’", "ã«", "ã®"

   "This is a great company" â†’ "great company"
   ```

3. **Remove Punctuation | å¥èª­ç‚¹é™¤å»**
   ```
   "AI, ML, and data!" â†’ "AI ML and data"
   ```

**Why | ãªãœ:**
- "Technology" and "technology" should be the SAME feature
- Common words like "the" don't help distinguish companies
- ã‚ˆã‚Šæ„å‘³ã®ã‚ã‚‹ç‰¹å¾´ã«ç„¦ç‚¹ã‚’å½“ã¦ã‚‹

---

## Slide 7: Step 4 - Stemming vs Lemmatization
## ã‚¹ãƒ©ã‚¤ãƒ‰7ï¼šã‚¹ãƒ†ãƒƒãƒ—4 - ã‚¹ãƒ†ãƒŸãƒ³ã‚° vs ãƒ¬ãƒ³ãƒåŒ–

### Reducing Words to Root Forms | å˜èªã‚’èªæ ¹å½¢å¼ã«é‚„å…ƒ

**Stemming (Fast, Rough) | ã‚¹ãƒ†ãƒŸãƒ³ã‚°ï¼ˆé€Ÿã„ã€å¤§ã¾ã‹ï¼‰:**
```
"running" â†’ "run"
"runner" â†’ "run"
"runs" â†’ "run"
"running" â†’ "run"  âœ…
"university" â†’ "univers"  âš ï¸ Not a real word!
```

**Lemmatization (Slower, Accurate) | ãƒ¬ãƒ³ãƒåŒ–ï¼ˆé…ã„ã€æ­£ç¢ºï¼‰:**
```
"running" â†’ "run"
"runner" â†’ "runner"  (different meaning)
"runs" â†’ "run"
"better" â†’ "good"  âœ… Understands meaning
"university" â†’ "university"  âœ… Keeps real words
```

**For This Course | ã“ã®ã‚³ãƒ¼ã‚¹ã§ã¯:**
Usually stemming is fine for simple classification tasks
é€šå¸¸ã€å˜ç´”ãªåˆ†é¡ã‚¿ã‚¹ã‚¯ã«ã¯ã‚¹ãƒ†ãƒŸãƒ³ã‚°ã§ååˆ†

---

## Slide 8: Step 5 - Bag of Words
## ã‚¹ãƒ©ã‚¤ãƒ‰8ï¼šã‚¹ãƒ†ãƒƒãƒ—5 - Bag of Words

### Converting Text to Numbers (Method 1)
### ãƒ†ã‚­ã‚¹ãƒˆã‚’æ•°å€¤ã«å¤‰æ›ï¼ˆæ–¹æ³•1ï¼‰

**Concept | ã‚³ãƒ³ã‚»ãƒ—ãƒˆ:**
Count how many times each word appears
å„å˜èªãŒä½•å›å‡ºç¾ã™ã‚‹ã‹ã‚’ã‚«ã‚¦ãƒ³ãƒˆ

**Example | ä¾‹:**
```
Company A: "AI and data solutions"
Company B: "Data analytics and AI"
Company C: "Financial services"

Vocabulary: ["ai", "analytics", "and", "data", "financial", "services", "solutions"]
èªå½™ï¼š["ai", "analytics", "and", "data", "financial", "services", "solutions"]

        ai  analytics  and  data  financial  services  solutions
A       1      0       1    1       0          0         1
B       1      1       1    1       0          0         0
C       0      0       0    0       1          1         0
```

**Strengths | å¼·ã¿:** Simple, interpretable | ã‚·ãƒ³ãƒ—ãƒ«ã€è§£é‡ˆå¯èƒ½
**Weaknesses | å¼±ã¿:** Ignores importance, creates sparse matrices | é‡è¦æ€§ã‚’ç„¡è¦–ã€ç–è¡Œåˆ—ã‚’ä½œæˆ

---

## Slide 9: TF-IDF - Smarter Features
## ã‚¹ãƒ©ã‚¤ãƒ‰9ï¼šTF-IDF - ã‚ˆã‚Šã‚¹ãƒãƒ¼ãƒˆãªç‰¹å¾´

### Weighting Words by Importance
### é‡è¦åº¦ã«ã‚ˆã‚‹å˜èªã®é‡ã¿ä»˜ã‘

**Problem with Bag of Words | Bag of Wordsã®å•é¡Œ:**
"company" appears in EVERY description â†’ not useful for distinguishing
ã€Œcompanyã€ãŒã™ã¹ã¦ã®èª¬æ˜ã«å‡ºç¾ â†’ åŒºåˆ¥ã«å½¹ç«‹ãŸãªã„

**TF-IDF Solution | TF-IDFè§£æ±ºç­–:**

**TF (Term Frequency) | ç”¨èªé »åº¦:**
How often does this word appear in THIS document?
ã“ã®å˜èªã¯ã“ã®æ–‡æ›¸ã§ä½•å›å‡ºç¾ã™ã‚‹ã‹ï¼Ÿ

**IDF (Inverse Document Frequency) | é€†æ–‡æ›¸é »åº¦:**
How RARE is this word across ALL documents?
ã“ã®å˜èªã¯ã™ã¹ã¦ã®æ–‡æ›¸ã§ã©ã‚Œã»ã©ç¨€ã‹ï¼Ÿ

**TF-IDF Score = TF Ã— IDF**

**Example | ä¾‹:**
```
Word "company":
  - TF: High (appears often in one document)
  - IDF: Low (appears in all documents)
  - TF-IDF: Low âŒ Not distinctive

Word "biotechnology":
  - TF: High (appears often in one document)
  - IDF: High (rare across documents)
  - TF-IDF: High âœ… Very distinctive!
```

---

## Slide 10: TF-IDF Intuition
## ã‚¹ãƒ©ã‚¤ãƒ‰10ï¼šTF-IDFç›´æ„Ÿ

### Visual Understanding | è¦–è¦šçš„ç†è§£

**Imagine 100 company descriptions:**
100ã®ä¼æ¥­èª¬æ˜ã‚’æƒ³åƒã—ã¦ãã ã•ã„ï¼š

```
Word: "company"
  Appears in: 95 companies
  TF-IDF: LOW (too common, not distinctive)

Word: "software"
  Appears in: 40 companies
  TF-IDF: MEDIUM (somewhat distinctive)

Word: "biotechnology"
  Appears in: 3 companies
  TF-IDF: HIGH (very distinctive!)
```

**Key Insight | é‡è¦ãªæ´å¯Ÿ:**
Words that are common in ONE document but rare ACROSS documents are most important
1ã¤ã®æ–‡æ›¸ã§ã¯ä¸€èˆ¬çš„ã ãŒã€æ–‡æ›¸å…¨ä½“ã§ã¯ç¨€ãªå˜èªãŒæœ€ã‚‚é‡è¦

---

## Slide 11: N-grams - Capturing Context
## ã‚¹ãƒ©ã‚¤ãƒ‰11ï¼šN-grams - ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®ã‚­ãƒ£ãƒ—ãƒãƒ£

### Beyond Single Words | å˜ä¸€å˜èªã‚’è¶…ãˆã¦

**Unigrams (1 word) | ãƒ¦ãƒ‹ã‚°ãƒ©ãƒ ï¼ˆ1å˜èªï¼‰:**
```
"machine learning solutions" â†’ ["machine", "learning", "solutions"]
```

**Bigrams (2 words) | ãƒã‚¤ã‚°ãƒ©ãƒ ï¼ˆ2å˜èªï¼‰:**
```
"machine learning solutions" â†’ ["machine learning", "learning solutions"]
```

**Trigrams (3 words) | ãƒˆãƒ©ã‚¤ã‚°ãƒ©ãƒ ï¼ˆ3å˜èªï¼‰:**
```
"machine learning solutions" â†’ ["machine learning solutions"]
```

**Why Use N-grams? | ãªãœN-gramsã‚’ä½¿ç”¨ã™ã‚‹ã‹ï¼Ÿ**
- Captures phrases: "machine learning" vs "machine" + "learning"
- Better context: "not good" has different meaning than "good"
- More features (but also more dimensionality)

**Trade-off | ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•:**
More context âœ… vs More features (curse of dimensionality) âŒ

---

## Slide 12: Japanese Text Processing
## ã‚¹ãƒ©ã‚¤ãƒ‰12ï¼šæ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†

### Special Considerations | ç‰¹åˆ¥ãªè€ƒæ…®äº‹é …

**Challenges | èª²é¡Œ:**

1. **No Spaces Between Words | å˜èªé–“ã«ã‚¹ãƒšãƒ¼ã‚¹ãŒãªã„**
   ```
   "æ©Ÿæ¢°å­¦ç¿’ã¯ç´ æ™´ã‚‰ã—ã„"
   Need: MeCab/Janome to tokenize
   å¿…è¦ï¼šMeCab/Janomeã§ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
   ```

2. **Multiple Character Types | è¤‡æ•°ã®æ–‡å­—ã‚¿ã‚¤ãƒ—**
   ```
   Hiragana: ã²ã‚‰ãŒãª
   Katakana: ã‚«ã‚¿ã‚«ãƒŠ
   Kanji: æ¼¢å­—
   Romaji: ABC
   ```

3. **Character Normalization | æ–‡å­—æ­£è¦åŒ–**
   ```
   Full-width: "ï¼¡ï¼¢ï¼£ï¼‘ï¼’ï¼“" â†’ Half-width: "ABC123"
   Katakana variations: "ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿" vs "ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ¼"
   ```

**Solution | è§£æ±ºç­–:**
```python
import MeCab
mecab = MeCab.Tagger()
text = "æ©Ÿæ¢°å­¦ç¿’ã¯ç´ æ™´ã‚‰ã—ã„"
tokens = mecab.parse(text).split()
# Result: ['æ©Ÿæ¢°å­¦ç¿’', 'ã¯', 'ç´ æ™´ã‚‰ã—ã„']
```

---

## Slide 13: Feature Matrix Dimensions
## ã‚¹ãƒ©ã‚¤ãƒ‰13ï¼šç‰¹å¾´è¡Œåˆ—ã®æ¬¡å…ƒ

### Understanding the Numbers | æ•°å€¤ã®ç†è§£

**Your Company Dataset:**
ã‚ãªãŸã®ä¼æ¥­ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼š

```
Number of companies: 30
Number of unique words (vocabulary): 500

Feature Matrix Shape: (30, 500)
ç‰¹å¾´è¡Œåˆ—å½¢çŠ¶ï¼š(30, 500)

30 rows (companies) Ã— 500 columns (features)
30è¡Œï¼ˆä¼æ¥­ï¼‰ Ã— 500åˆ—ï¼ˆç‰¹å¾´ï¼‰
```

**Sparse Matrix Problem | ç–è¡Œåˆ—å•é¡Œ:**
```
Most cells are ZERO (word doesn't appear in that document)
ã»ã¨ã‚“ã©ã®ã‚»ãƒ«ã¯ã‚¼ãƒ­ï¼ˆãã®æ–‡æ›¸ã«å˜èªãŒå‡ºç¾ã—ãªã„ï¼‰

Example row: [0, 0, 0.5, 0, 0, 0, 0, 0.3, 0, 0, 0, ...]
             95% zeros!
```

**Solutions | è§£æ±ºç­–:**
- Limit vocabulary size: `max_features=100`
- Remove rare words: `min_df=2` (appear in at least 2 documents)
- Remove common words: `max_df=0.8` (appear in max 80% of documents)

---

## Slide 14: Practical Code Example
## ã‚¹ãƒ©ã‚¤ãƒ‰14ï¼šå®Ÿç”¨çš„ãªã‚³ãƒ¼ãƒ‰ä¾‹

### From Text to Features in Python
### Pythonã§ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ç‰¹å¾´ã¸

```python
from sklearn.feature_extraction.text import TfidfVectorizer

# Sample company descriptions
companies = [
    "Leading AI and machine learning company",
    "Traditional manufacturing and industrial solutions",
    "AI-powered data analytics platform"
]

# Create TF-IDF vectorizer
vectorizer = TfidfVectorizer(
    max_features=50,      # Limit to 50 most important words
    ngram_range=(1, 2),   # Use unigrams and bigrams
    min_df=1              # Word must appear in at least 1 doc
)

# Transform text to features
features = vectorizer.fit_transform(companies)

# Result: (3, 50) matrix - 3 companies, 50 features
print(features.shape)  # (3, 50)

# See vocabulary
print(vectorizer.get_feature_names_out())
# ['ai', 'analytics', 'company', 'data', 'learning',
#  'machine', 'ai powered', 'machine learning', ...]
```

---

## Slide 15: Today's Activity Preview
## ã‚¹ãƒ©ã‚¤ãƒ‰15ï¼šä»Šæ—¥ã®ã‚¢ã‚¯ãƒ†ã‚£ãƒ“ãƒ†ã‚£ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼

### What You'll Do | ä»Šæ—¥ã™ã‚‹ã“ã¨

**40-60 Minute Hands-On Activity:**

1. **Load Company Data** (10 min)
   - Import your collected company descriptions
   - ä¼æ¥­èª¬æ˜ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ

2. **Text Preprocessing** (15 min)
   - Clean, tokenize, normalize text
   - ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã€ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã€æ­£è¦åŒ–

3. **Feature Extraction** (15 min)
   - Apply Bag of Words and TF-IDF
   - Bag of Wordsã¨TF-IDFã‚’é©ç”¨

4. **Feature Analysis** (10 min)
   - Examine important words for each company type
   - å„ä¼æ¥­ã‚¿ã‚¤ãƒ—ã®é‡è¦ãªå˜èªã‚’èª¿æŸ»

5. **Save for ML** (10 min)
   - Export features for next week's classification
   - æ¥é€±ã®åˆ†é¡ã®ãŸã‚ã«ç‰¹å¾´ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ

**The ML-101 Bot will guide you every step!**
**ML-101ãƒœãƒƒãƒˆãŒå„ã‚¹ãƒ†ãƒƒãƒ—ã§ã‚¬ã‚¤ãƒ‰ã—ã¾ã™ï¼**

---

## Slide 16: Connection to Course Project
## ã‚¹ãƒ©ã‚¤ãƒ‰16ï¼šã‚³ãƒ¼ã‚¹ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¸ã®æ¥ç¶š

### Why This Matters | ãªãœã“ã‚ŒãŒé‡è¦ã‹

**The Big Picture | å…¨ä½“åƒ:**

```
Week 1-5: Collected company descriptions (TEXT)
ç¬¬1-5é€±ï¼šä¼æ¥­èª¬æ˜ã‚’åé›†ï¼ˆãƒ†ã‚­ã‚¹ãƒˆï¼‰

    â†“

Week 6 TODAY: Convert text to features (NUMBERS)
ç¬¬6é€±ä»Šæ—¥ï¼šãƒ†ã‚­ã‚¹ãƒˆã‚’ç‰¹å¾´ã«å¤‰æ›ï¼ˆæ•°å€¤ï¼‰

    â†“

Week 7+: Train ML classifier (PREDICTIONS)
ç¬¬7é€±ä»¥é™ï¼šMLåˆ†é¡å™¨ã‚’è¨“ç·´ï¼ˆäºˆæ¸¬ï¼‰
```

**Your Goal | ã‚ãªãŸã®ç›®æ¨™:**
Convert company descriptions into numerical features that capture:
ä¼æ¥­èª¬æ˜ã‚’æ¬¡ã®å†…å®¹ã‚’æ‰ãˆãŸæ•°å€¤ç‰¹å¾´ã«å¤‰æ›ï¼š
- Industry type (tech vs finance vs manufacturing)
- Company focus (AI, data, traditional business)
- Language patterns that indicate your interests

---

## Slide 17: Common Mistakes to Avoid
## ã‚¹ãƒ©ã‚¤ãƒ‰17ï¼šé¿ã‘ã‚‹ã¹ãä¸€èˆ¬çš„ãªé–“é•ã„

### Text Processing Pitfalls | ãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†ã®è½ã¨ã—ç©´

âŒ **Mistake 1: Not tokenizing Japanese text**
```python
# Wrong: Using default tokenizer for Japanese
text = "æ©Ÿæ¢°å­¦ç¿’ã¯ç´ æ™´ã‚‰ã—ã„"
tokens = text.split()  # Doesn't work - no spaces!
```

âŒ **Mistake 2: Creating too many features**
```python
# Wrong: No feature limit
vectorizer = TfidfVectorizer()  # Might create 10,000+ features!
```

âŒ **Mistake 3: Not saving the vectorizer**
```python
# Wrong: Can't process new data later
features = TfidfVectorizer().fit_transform(text)
# The vectorizer is lost! Can't transform new companies!
```

âœ… **Correct Approach:**
```python
# Right: Save vectorizer for reuse
vectorizer = TfidfVectorizer(max_features=100)
features = vectorizer.fit_transform(text)
# Save: pickle.dump(vectorizer, file)
```

---

## Slide 18: Key Takeaways
## ã‚¹ãƒ©ã‚¤ãƒ‰18ï¼šé‡è¦ãªãƒã‚¤ãƒ³ãƒˆ

### Remember | è¦šãˆã¦ãŠã„ã¦ãã ã•ã„

âœ… **ML needs numbers, not text**
   æ©Ÿæ¢°å­¦ç¿’ã¯æ•°å€¤ãŒå¿…è¦ã€ãƒ†ã‚­ã‚¹ãƒˆã§ã¯ãªã„

âœ… **Text preprocessing pipeline: Clean â†’ Tokenize â†’ Normalize â†’ Vectorize**
   ãƒ†ã‚­ã‚¹ãƒˆå‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼šã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚° â†’ ãƒˆãƒ¼ã‚¯ãƒ³åŒ– â†’ æ­£è¦åŒ– â†’ ãƒ™ã‚¯ãƒˆãƒ«åŒ–

âœ… **TF-IDF highlights distinctive words**
   TF-IDFã¯ç‰¹å¾´çš„ãªå˜èªã‚’å¼·èª¿ã™ã‚‹

âœ… **Japanese text needs special tokenizers (MeCab, Janome)**
   æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆã«ã¯ç‰¹åˆ¥ãªãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãŒå¿…è¦ï¼ˆMeCabã€Janomeï¼‰

âœ… **Save your vectorizer for reuse!**
   å†åˆ©ç”¨ã®ãŸã‚ã«ãƒ™ã‚¯ãƒˆãƒ©ã‚¤ã‚¶ã‚’ä¿å­˜ã—ã¦ãã ã•ã„ï¼

âœ… **Balance: Enough features to capture meaning, not so many that you have noise**
   ãƒãƒ©ãƒ³ã‚¹ï¼šæ„å‘³ã‚’æ‰ãˆã‚‹ã®ã«ååˆ†ãªç‰¹å¾´ã€ãƒã‚¤ã‚ºã«ãªã‚‹ã»ã©å¤šããªã„

---

## Slide 19: Let's Get Started!
## ã‚¹ãƒ©ã‚¤ãƒ‰19ï¼šå§‹ã‚ã¾ã—ã‚‡ã†ï¼

**"Now let's transform your company descriptions into ML-ready features!"**
**ã€Œã§ã¯ã€ã‚ãªãŸã®ä¼æ¥­èª¬æ˜ã‚’MLå¯¾å¿œã®ç‰¹å¾´ã«å¤‰æ›ã—ã¾ã—ã‚‡ã†ï¼ã€**

### Next Steps | æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

1. Open the ML-101 Bot portal
   ML-101ãƒœãƒƒãƒˆãƒãƒ¼ã‚¿ãƒ«ã‚’é–‹ã

2. Load your company dataset
   ä¼æ¥­ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒ­ãƒ¼ãƒ‰

3. Begin text preprocessing pipeline
   ãƒ†ã‚­ã‚¹ãƒˆå‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’é–‹å§‹

4. Extract TF-IDF features
   TF-IDFç‰¹å¾´ã‚’æŠ½å‡º

5. Save features for next week!
   æ¥é€±ã®ãŸã‚ã«ç‰¹å¾´ã‚’ä¿å­˜ï¼

**Remember:** Good preprocessing = Better classification!
**è¦šãˆã¦ãŠã„ã¦ãã ã•ã„:** è‰¯ã„å‰å‡¦ç† = ã‚ˆã‚Šè‰¯ã„åˆ†é¡ï¼

---

## Notes for Instructor | ã‚¤ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚¿ãƒ¼å‘ã‘ãƒ¡ãƒ¢

### Delivery Tips

- **Timing:** 15-20 minutes maximum - focus on core concepts
- **Live Demo:** Show TF-IDF calculation with 2-3 real company descriptions
- **Engagement:** Ask "What words would distinguish a tech company from a bank?"
- **Visualization:** Draw the feature matrix on board to show dimensions
- **Japanese Context:** Emphasize tokenization challenge for Japanese students

### Common Questions

Q: "Should I remove all numbers from text?"
A: "Depends! '2020' might not help, but '24/7' or 'Fortune 500' might be meaningful."

Q: "How many features should I create?"
A: "Start with 50-100. Too few = lose information. Too many = noise and overfitting."

Q: "Do I need to understand the TF-IDF math?"
A: "Not the formula details! Understand the intuition: important = common in document, rare overall."

Q: "Can I use English tokenization for Japanese text?"
A: "No! You must use MeCab or Janome. English tokenizers split by spaces - Japanese has no spaces."

### After Lecture

- Students work with bot on text preprocessing
- Circulate to help with Japanese tokenizer installation
- Check that students are saving vectorizer for reuse
- Validate feature matrix dimensions are reasonable

### Technical Setup Note

**Before Class**: Verify MeCab/Janome installation instructions work on student systems
**Alternative**: Have Janome ready as backup (easier installation than MeCab)

---

**End of Week 6 Lecture Slides**
**ç¬¬6é€±è¬›ç¾©ã‚¹ãƒ©ã‚¤ãƒ‰çµ‚äº†**

*Generated for ML-101 Course*
*ML-101ã‚³ãƒ¼ã‚¹ç”¨ã«ç”Ÿæˆ*
