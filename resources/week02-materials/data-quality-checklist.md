# Data Quality Checklist
# ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

**Week 2 Reference Material | ç¬¬2é€±å‚è€ƒè³‡æ–™**
**Course:** ML-101 Machine Learning and Intelligence

---

## Introduction | ã¯ã˜ã‚ã«

**"Garbage in, garbage out" - The #1 rule of Machine Learning**
**ã€Œã‚´ãƒŸã‚’å…¥ã‚Œã‚Œã°ã‚´ãƒŸãŒå‡ºã‚‹ã€- æ©Ÿæ¢°å­¦ç¿’ã®ç¬¬1ãƒ«ãƒ¼ãƒ«**

Poor quality data = Poor quality predictions, no matter how good your model is!
ä½å“è³ªãƒ‡ãƒ¼ã‚¿ = ä½å“è³ªäºˆæ¸¬ã€ãƒ¢ãƒ‡ãƒ«ãŒã©ã‚Œã ã‘è‰¯ãã¦ã‚‚ï¼

This checklist helps you identify and understand data quality issues before training ML models.
ã“ã®ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆã¯ã€MLãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã™ã‚‹å‰ã«ãƒ‡ãƒ¼ã‚¿å“è³ªã®å•é¡Œã‚’ç‰¹å®šã—ã€ç†è§£ã™ã‚‹ã®ã«å½¹ç«‹ã¡ã¾ã™ã€‚

---

## The 5 Main Data Quality Issues | 5ã¤ã®ä¸»ãªãƒ‡ãƒ¼ã‚¿å“è³ªå•é¡Œ

### Quick Overview | ã‚¯ã‚¤ãƒƒã‚¯æ¦‚è¦

```
1. Missing Values      â“ Data points that don't exist
   æ¬ æå€¤               å­˜åœ¨ã—ãªã„ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆ

2. Duplicates          ğŸ‘¯ Same record appears multiple times
   é‡è¤‡                åŒã˜ãƒ¬ã‚³ãƒ¼ãƒ‰ãŒè¤‡æ•°å›è¡¨ç¤ºã•ã‚Œã‚‹

3. Inconsistencies     ğŸ”€ Same thing represented differently
   ä¸æ•´åˆ              åŒã˜ã‚‚ã®ãŒç•°ãªã‚‹æ–¹æ³•ã§è¡¨ã•ã‚Œã‚‹

4. Invalid Values      âš ï¸ Data that doesn't make sense
   ç„¡åŠ¹ãªå€¤            æ„å‘³ã‚’ãªã•ãªã„ãƒ‡ãƒ¼ã‚¿

5. Outdated Data       ğŸ“… Information that's no longer current
   å¤ã„ãƒ‡ãƒ¼ã‚¿          ã‚‚ã¯ã‚„æœ€æ–°ã§ã¯ãªã„æƒ…å ±
```

---

## 1. Missing Values | æ¬ æå€¤

### What to Look For | ä½•ã‚’æ¢ã™ã‹

**Signs of missing data:**
**æ¬ æãƒ‡ãƒ¼ã‚¿ã®å…†å€™:**

âœ“ Empty cells in spreadsheets
  ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã®ç©ºã®ã‚»ãƒ«

âœ“ "N/A", "NULL", "None", "â€”" entries
  ã€ŒN/Aã€ã€ã€ŒNULLã€ã€ã€ŒNoneã€ã€ã€Œâ€”ã€ã‚¨ãƒ³ãƒˆãƒª

âœ“ Zeros that should be numbers (e.g., salary = 0)
  æ•°å€¤ã§ã‚ã‚‹ã¹ãã‚¼ãƒ­ï¼ˆä¾‹ï¼šçµ¦ä¸ = 0ï¼‰

âœ“ Default values like "Unknown" or "Not specified"
  ã€Œä¸æ˜ã€ã¾ãŸã¯ã€ŒæŒ‡å®šãªã—ã€ãªã©ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤

---

### How to Identify | ç‰¹å®šæ–¹æ³•

**Visual inspection:**
**ç›®è¦–æ¤œæŸ»:**
```
Company Name | Industry | Employee Count | Revenue
ABC Corp     | Tech     | 500           | $10M
XYZ Inc      | Finance  |               | $5M    â† Missing!
LMN Ltd      | Tech     | 200           |        â† Missing!
```

**In Python/Pandas:**
```python
import pandas as pd

# Check for missing values
df.isnull().sum()
# Output shows count of missing values per column

# Visualize missing data
df.info()
# Shows data types and non-null counts
```

**Quick calculation:**
```
Missing percentage = (Missing count / Total count) Ã— 100

Example:
10 missing values out of 100 rows = 10% missing
```

---

### Impact on ML | MLã¸ã®å½±éŸ¿

**Why missing values are problematic:**
**ãªãœæ¬ æå€¤ãŒå•é¡Œãªã®ã‹:**

âŒ **Model can't train** - Many algorithms reject incomplete data
   ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã§ããªã„ - å¤šãã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ãŒä¸å®Œå…¨ãªãƒ‡ãƒ¼ã‚¿ã‚’æ‹’å¦

âŒ **Reduced dataset size** - Deleting rows loses valuable information
   ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚µã‚¤ã‚ºã®å‰Šæ¸› - è¡Œã‚’å‰Šé™¤ã™ã‚‹ã¨è²´é‡ãªæƒ…å ±ãŒå¤±ã‚ã‚Œã‚‹

âŒ **Biased predictions** - If missing data follows a pattern
   åã£ãŸäºˆæ¸¬ - æ¬ æãƒ‡ãƒ¼ã‚¿ãŒãƒ‘ã‚¿ãƒ¼ãƒ³ã«å¾“ã†å ´åˆ

**Example:**
If only high-salary companies don't report salary, your model won't learn about high-paying jobs!
é«˜çµ¦ä¼æ¥­ã®ã¿ãŒçµ¦ä¸ã‚’å ±å‘Šã—ãªã„å ´åˆã€ãƒ¢ãƒ‡ãƒ«ã¯é«˜çµ¦ã®ä»•äº‹ã«ã¤ã„ã¦å­¦ç¿’ã—ã¾ã›ã‚“ï¼

---

### Common Solutions | ä¸€èˆ¬çš„ãªè§£æ±ºç­–

| Solution | When to Use | Pros | Cons |
|----------|-------------|------|------|
| **Delete rows** | <10% missing, random | Simple, clean data | Lose information |
| **Delete columns** | >50% missing in one column | Keep other data intact | Lose entire feature |
| **Fill with average** | Numeric data, random missing | Keeps dataset size | May distort distribution |
| **Fill with mode** | Categorical data | Simple, logical | May create false patterns |
| **Advanced imputation** | Important features, patterns | More accurate | Complex, time-consuming |

| è§£æ±ºç­– | ã„ã¤ä½¿ç”¨ã™ã‚‹ã‹ | é•·æ‰€ | çŸ­æ‰€ |
|--------|---------------|------|------|
| **è¡Œã‚’å‰Šé™¤** | <10%æ¬ æã€ãƒ©ãƒ³ãƒ€ãƒ  | ã‚·ãƒ³ãƒ—ãƒ«ã€ã‚¯ãƒªãƒ¼ãƒ³ãªãƒ‡ãƒ¼ã‚¿ | æƒ…å ±ã‚’å¤±ã† |
| **åˆ—ã‚’å‰Šé™¤** | 1åˆ—ã«>50%æ¬ æ | ä»–ã®ãƒ‡ãƒ¼ã‚¿ã‚’ãã®ã¾ã¾ä¿æŒ | å…¨ä½“ã®ç‰¹å¾´ã‚’å¤±ã† |
| **å¹³å‡ã§åŸ‹ã‚ã‚‹** | æ•°å€¤ãƒ‡ãƒ¼ã‚¿ã€ãƒ©ãƒ³ãƒ€ãƒ æ¬ æ | ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚µã‚¤ã‚ºã‚’ç¶­æŒ | åˆ†å¸ƒã‚’æ­ªã‚ã‚‹å¯èƒ½æ€§ |
| **æœ€é »å€¤ã§åŸ‹ã‚ã‚‹** | ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«ãƒ‡ãƒ¼ã‚¿ | ã‚·ãƒ³ãƒ—ãƒ«ã€è«–ç†çš„ | å½ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ä½œæˆã™ã‚‹å¯èƒ½æ€§ |
| **é«˜åº¦ãªè£œå®Œ** | é‡è¦ãªç‰¹å¾´ã€ãƒ‘ã‚¿ãƒ¼ãƒ³ | ã‚ˆã‚Šæ­£ç¢º | è¤‡é›‘ã€æ™‚é–“ãŒã‹ã‹ã‚‹ |

**Decision flowchart:**
**æ±ºå®šãƒ•ãƒ­ãƒ¼ãƒãƒ£ãƒ¼ãƒˆ:**
```
How much data is missing?
ã©ã‚Œã ã‘ã®ãƒ‡ãƒ¼ã‚¿ãŒæ¬ æã—ã¦ã„ã¾ã™ã‹ï¼Ÿ

< 5% â†’ Delete rows (safest)
       è¡Œã‚’å‰Šé™¤ï¼ˆæœ€ã‚‚å®‰å…¨ï¼‰

5-20% â†’ Consider filling
        åŸ‹ã‚ã‚‹ã“ã¨ã‚’æ¤œè¨

> 50% in one column â†’ Delete column
                      åˆ—ã‚’å‰Šé™¤

Critical feature? â†’ Advanced imputation
é‡è¦ãªç‰¹å¾´ï¼Ÿ      é«˜åº¦ãªè£œå®Œ
```

---

## 2. Duplicates | é‡è¤‡

### What to Look For | ä½•ã‚’æ¢ã™ã‹

**Types of duplicates:**
**é‡è¤‡ã®ã‚¿ã‚¤ãƒ—:**

**Exact duplicates:**
**å®Œå…¨ãªé‡è¤‡:**
```
Company Name | Industry | Employee Count
ABC Corp     | Tech     | 500
ABC Corp     | Tech     | 500    â† Exact duplicate!
```

**Partial duplicates (tricky!):**
**éƒ¨åˆ†çš„ãªé‡è¤‡ï¼ˆå„ä»‹ï¼ï¼‰:**
```
Company Name      | Industry | Employee Count
ABC Corporation   | Tech     | 500
ABC Corp          | Tech     | 500    â† Same company, different name!
ABC Corp.         | Tech     | 501    â† Slightly different data!
```

---

### How to Identify | ç‰¹å®šæ–¹æ³•

**Visual inspection:**
**ç›®è¦–æ¤œæŸ»:**
- Sort by company name and look for repeats
  ä¼šç¤¾åã§ã‚½ãƒ¼ãƒˆã—ã¦ç¹°ã‚Šè¿”ã—ã‚’æ¢ã™
- Check for similar names with small variations
  å°ã•ãªå¤‰åŒ–ã‚’æŒã¤é¡ä¼¼ã—ãŸåå‰ã‚’ç¢ºèª

**In Python/Pandas:**
```python
# Find exact duplicates
df.duplicated().sum()
# Shows count of duplicate rows

# View duplicate rows
df[df.duplicated(keep=False)]
# Shows all duplicate entries

# Check for duplicates in specific column
df[df.duplicated(subset=['Company Name'], keep=False)]
# Finds duplicates based on company name only
```

**Manual checks:**
- Are there two entries for the same company?
  åŒã˜ä¼šç¤¾ã«2ã¤ã®ã‚¨ãƒ³ãƒˆãƒªãŒã‚ã‚Šã¾ã™ã‹ï¼Ÿ
- Does the total count seem too high?
  åˆè¨ˆã‚«ã‚¦ãƒ³ãƒˆãŒé«˜ã™ãã‚‹ã‚ˆã†ã«è¦‹ãˆã¾ã™ã‹ï¼Ÿ
- Check known companies - do they appear once or multiple times?
  æ—¢çŸ¥ã®ä¼æ¥­ã‚’ç¢ºèª - 1å›ã¾ãŸã¯è¤‡æ•°å›è¡¨ç¤ºã•ã‚Œã¾ã™ã‹ï¼Ÿ

---

### Impact on ML | MLã¸ã®å½±éŸ¿

**Why duplicates are problematic:**
**ãªãœé‡è¤‡ãŒå•é¡Œãªã®ã‹:**

âŒ **Model learns the same example multiple times**
   ãƒ¢ãƒ‡ãƒ«ãŒåŒã˜ä¾‹ã‚’è¤‡æ•°å›å­¦ç¿’ã™ã‚‹
   - Gives duplicate data more "weight"
   - é‡è¤‡ãƒ‡ãƒ¼ã‚¿ã«å¤šãã®ã€Œé‡ã¿ã€ã‚’ä¸ãˆã‚‹

âŒ **Inflates dataset size artificially**
   ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚µã‚¤ã‚ºã‚’äººç‚ºçš„ã«è†¨ã‚‰ã¾ã›ã‚‹
   - Makes you think you have more data than you do
   - å®Ÿéš›ã‚ˆã‚Šã‚‚å¤šãã®ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹ã¨æ€ã‚ã›ã‚‹

âŒ **Distorts statistics and patterns**
   çµ±è¨ˆã¨ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ­ªã‚ã‚‹
   - Average, counts, percentages all wrong
   - å¹³å‡ã€ã‚«ã‚¦ãƒ³ãƒˆã€ãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸ãŒã™ã¹ã¦é–“é•ã£ã¦ã„ã‚‹

**Example:**
```
If "ABC Corp" appears 10 times but others appear once:
Model learns: "Tech companies with 500 employees are common!"
Reality: Only 1 such company, just duplicated data

ã€ŒABC Corpã€ãŒ10å›è¡¨ç¤ºã•ã‚Œã‚‹ãŒä»–ã¯1å›ã®å ´åˆï¼š
ãƒ¢ãƒ‡ãƒ«ã¯å­¦ç¿’ï¼šã€Œ500äººã®å¾“æ¥­å“¡ã‚’æŒã¤ãƒ†ãƒƒã‚¯ä¼æ¥­ã¯ä¸€èˆ¬çš„ï¼ã€
ç¾å®Ÿï¼šãã®ã‚ˆã†ãªä¼æ¥­ã¯1ã¤ã ã‘ã€é‡è¤‡ãƒ‡ãƒ¼ã‚¿ã®ã¿
```

---

### Common Solutions | ä¸€èˆ¬çš„ãªè§£æ±ºç­–

**1. Remove exact duplicates:**
**1. å®Œå…¨ãªé‡è¤‡ã‚’å‰Šé™¤:**
```python
# Keep first occurrence, remove rest
df = df.drop_duplicates()

# Keep last occurrence
df = df.drop_duplicates(keep='last')
```
âœ… **Always do this!** No reason to keep exact copies.
âœ… **å¸¸ã«ã“ã‚Œã‚’è¡Œã£ã¦ãã ã•ã„ï¼** æ­£ç¢ºãªã‚³ãƒ”ãƒ¼ã‚’ä¿æŒã™ã‚‹ç†ç”±ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚

---

**2. Handle partial duplicates:**
**2. éƒ¨åˆ†çš„ãªé‡è¤‡ã‚’å‡¦ç†:**

**Manual review:**
- Check if entries are truly the same company
  ã‚¨ãƒ³ãƒˆãƒªãŒæœ¬å½“ã«åŒã˜ä¼šç¤¾ã‹ã©ã†ã‹ã‚’ç¢ºèª
- Decide which record to keep (most complete? most recent?)
  ã©ã®ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’ä¿æŒã™ã‚‹ã‹ã‚’æ±ºå®šï¼ˆæœ€ã‚‚å®Œå…¨ï¼Ÿæœ€æ–°ï¼Ÿï¼‰
- Standardize names first
  æœ€åˆã«åå‰ã‚’æ¨™æº–åŒ–

**Standardization example:**
**æ¨™æº–åŒ–ã®ä¾‹:**
```
Before:
- ABC Corp
- ABC Corporation
- ABC Corp.

After (standardized):
- ABC Corp
- ABC Corp
- ABC Corp

Then remove duplicates!
ãã®å¾Œé‡è¤‡ã‚’å‰Šé™¤ï¼
```

---

## 3. Inconsistencies | ä¸æ•´åˆ

### What to Look For | ä½•ã‚’æ¢ã™ã‹

**Common inconsistency types:**
**ä¸€èˆ¬çš„ãªä¸æ•´åˆã‚¿ã‚¤ãƒ—:**

**Format inconsistencies:**
**ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®ä¸æ•´åˆ:**
```
Date formats:
2024-01-15
01/15/2024
January 15, 2024
15-Jan-24       â† All the same date, different formats!
```

**Capitalization/spelling:**
**å¤§æ–‡å­—åŒ–/ã‚¹ãƒšãƒ«:**
```
Industry:
Tech
tech
Technology
Information Technology  â† All mean the same thing!
```

**Units:**
**å˜ä½:**
```
Employee Count:
500
500 employees
0.5K
Five hundred     â† Same number, different representations!
```

**Abbreviations:**
**ç•¥èª:**
```
Location:
Tokyo
Tokyo, Japan
Tokyo, JP
TYO
æ±äº¬            â† All the same city!
```

---

### How to Identify | ç‰¹å®šæ–¹æ³•

**Check unique values:**
**ä¸€æ„ã®å€¤ã‚’ç¢ºèª:**
```python
# See all unique values in a column
df['Industry'].unique()
# Output: ['Tech', 'tech', 'Technology', 'Finance', ...]

# Count occurrences of each unique value
df['Industry'].value_counts()
# Shows frequency of each variation
```

**Visual patterns to spot:**
**è¦‹ã¤ã‘ã‚‹ã¹ãè¦–è¦šçš„ãƒ‘ã‚¿ãƒ¼ãƒ³:**
- Similar values with small differences
  å°ã•ãªé•ã„ã®ã‚ã‚‹é¡ä¼¼ã®å€¤
- Same concept, different words
  åŒã˜æ¦‚å¿µã€ç•°ãªã‚‹å˜èª
- Mixed languages (English/Japanese)
  æ··åˆè¨€èªï¼ˆè‹±èª/æ—¥æœ¬èªï¼‰
- Different date/number formats
  ç•°ãªã‚‹æ—¥ä»˜/æ•°å€¤ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ

**Manual checks:**
**æ‰‹å‹•ãƒã‚§ãƒƒã‚¯:**
- Sort column alphabetically - similar values group together
  åˆ—ã‚’ã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆé †ã«ã‚½ãƒ¼ãƒˆ - é¡ä¼¼ã®å€¤ãŒã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã•ã‚Œã‚‹
- Check for typos or misspellings
  èª¤å­—ã‚„ã‚¹ãƒšãƒ«ãƒŸã‚¹ã‚’ç¢ºèª
- Look for variations of the same thing
  åŒã˜ã‚‚ã®ã®å¤‰ç¨®ã‚’æ¢ã™

---

### Impact on ML | MLã¸ã®å½±éŸ¿

**Why inconsistencies are problematic:**
**ãªãœä¸æ•´åˆãŒå•é¡Œãªã®ã‹:**

âŒ **Model treats same thing as different**
   ãƒ¢ãƒ‡ãƒ«ãŒåŒã˜ã‚‚ã®ã‚’ç•°ãªã‚‹ã‚‚ã®ã¨ã—ã¦æ‰±ã†
   - "Tech" and "tech" seen as two separate industries!
   - ã€ŒTechã€ã¨ã€Œtechã€ãŒ2ã¤ã®åˆ¥ã€…ã®æ¥­ç•Œã¨ã—ã¦è¦‹ã‚‰ã‚Œã‚‹ï¼

âŒ **Splits data unnecessarily**
   ãƒ‡ãƒ¼ã‚¿ã‚’ä¸å¿…è¦ã«åˆ†å‰²
   - Instead of 100 "Tech" companies, you have 50 "Tech" + 30 "tech" + 20 "Technology"
   - 100ã®ã€ŒTechã€ä¼æ¥­ã®ä»£ã‚ã‚Šã«ã€50ã®ã€ŒTechã€+ 30ã®ã€Œtechã€+ 20ã®ã€ŒTechnologyã€ãŒã‚ã‚‹

âŒ **Model can't learn patterns correctly**
   ãƒ¢ãƒ‡ãƒ«ãŒãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ­£ã—ãå­¦ç¿’ã§ããªã„
   - Dilutes signal across multiple categories
   - è¤‡æ•°ã®ã‚«ãƒ†ã‚´ãƒªãƒ¼ã«ã‚ãŸã£ã¦ã‚·ã‚°ãƒŠãƒ«ã‚’å¸Œé‡ˆã™ã‚‹

**Example:**
```
Inconsistent data:
- "Tokyo" companies: 30
- "æ±äº¬" companies: 25
- "Tokyo, Japan" companies: 20

Model thinks: Three different locations!
Reality: All Tokyo! Should be 75 companies total.

ä¸æ•´åˆãƒ‡ãƒ¼ã‚¿ï¼š
- ã€ŒTokyoã€ä¼æ¥­ï¼š30
- ã€Œæ±äº¬ã€ä¼æ¥­ï¼š25
- ã€ŒTokyo, Japanã€ä¼æ¥­ï¼š20

ãƒ¢ãƒ‡ãƒ«ã¯è€ƒãˆã‚‹ï¼š3ã¤ã®ç•°ãªã‚‹å ´æ‰€ï¼
ç¾å®Ÿï¼šã™ã¹ã¦æ±äº¬ï¼åˆè¨ˆ75ä¼æ¥­ã§ã‚ã‚‹ã¹ãã€‚
```

---

### Common Solutions | ä¸€èˆ¬çš„ãªè§£æ±ºç­–

**1. Standardize text:**
**1. ãƒ†ã‚­ã‚¹ãƒˆã‚’æ¨™æº–åŒ–:**
```python
# Convert to lowercase
df['Industry'] = df['Industry'].str.lower()

# Remove extra spaces
df['Industry'] = df['Industry'].str.strip()

# Standardize abbreviations
df['Industry'] = df['Industry'].replace({
    'tech': 'Technology',
    'info tech': 'Technology',
    'IT': 'Technology'
})
```

---

**2. Standardize formats:**
**2. ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’æ¨™æº–åŒ–:**
```python
# Convert dates to standard format
df['Date'] = pd.to_datetime(df['Date'])

# Standardize numbers (remove commas, text)
df['Employee Count'] = df['Employee Count'].str.replace(',', '')
df['Employee Count'] = pd.to_numeric(df['Employee Count'])
```

---

**3. Create mapping rules:**
**3. ãƒãƒƒãƒ”ãƒ³ã‚°ãƒ«ãƒ¼ãƒ«ã‚’ä½œæˆ:**
```python
# Map variations to standard values
industry_mapping = {
    'tech': 'Technology',
    'technology': 'Technology',
    'information technology': 'Technology',
    'IT': 'Technology',
    'fin': 'Finance',
    'financial services': 'Finance'
}

df['Industry'] = df['Industry'].map(industry_mapping)
```

**Best practice:**
**ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹:**
âœ… Choose ONE standard format for each field
  å„ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã«1ã¤ã®æ¨™æº–ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’é¸æŠ
âœ… Document your standardization rules
  æ¨™æº–åŒ–ãƒ«ãƒ¼ãƒ«ã‚’æ–‡æ›¸åŒ–
âœ… Apply consistently across entire dataset
  ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå…¨ä½“ã«ä¸€è²«ã—ã¦é©ç”¨

---

## 4. Invalid Values | ç„¡åŠ¹ãªå€¤

### What to Look For | ä½•ã‚’æ¢ã™ã‹

**Values that don't make logical sense:**
**è«–ç†çš„ã«æ„å‘³ã‚’ãªã•ãªã„å€¤:**

**Impossible numbers:**
**ä¸å¯èƒ½ãªæ•°å€¤:**
```
Age: -5 years          â† Can't be negative!
Employee Count: 0      â† Company must have at least 1 employee
Salary: $0            â† Unlikely for full-time job
Temperature: 500Â°C     â† Too hot to be real
```

**Out-of-range values:**
**ç¯„å›²å¤–ã®å€¤:**
```
Expected: 0-100
Actual: 150           â† Outside valid range!

Month: 13             â† Only 12 months!
Percentage: 150%      â† Can't exceed 100% (usually)
```

**Wrong data type:**
**é–“é•ã£ãŸãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—:**
```
Expected: Number
Actual: "Five hundred"  â† Text instead of number!

Expected: Date
Actual: "Next week"     â† Vague, not specific date!
```

**Categorical mismatches:**
**ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«ãƒŸã‚¹ãƒãƒƒãƒ:**
```
Industry options: Tech, Finance, Healthcare, Retail
Actual value: "Purple"          â† Not a valid industry!
Actual value: "123"             â† Should be text, not number
```

---

### How to Identify | ç‰¹å®šæ–¹æ³•

**Statistical checks:**
**çµ±è¨ˆãƒã‚§ãƒƒã‚¯:**
```python
# Check min/max values
df['Employee Count'].min()  # Should be > 0
df['Employee Count'].max()  # Should be reasonable

# View summary statistics
df.describe()
# Look for suspicious min/max values

# Check for negative values where they shouldn't exist
(df['Salary'] < 0).sum()
```

**Visual inspection:**
**ç›®è¦–æ¤œæŸ»:**
- Sort column by value - extremes appear at top/bottom
  å€¤ã§åˆ—ã‚’ã‚½ãƒ¼ãƒˆ - æ¥µç«¯ãªå€¤ãŒä¸Š/ä¸‹ã«è¡¨ç¤ºã•ã‚Œã‚‹
- Look for outliers (values very different from others)
  å¤–ã‚Œå€¤ã‚’æ¢ã™ï¼ˆä»–ã¨ã¯éå¸¸ã«ç•°ãªã‚‹å€¤ï¼‰
- Check if values match expected categories
  å€¤ãŒæœŸå¾…ã•ã‚Œã‚‹ã‚«ãƒ†ã‚´ãƒªãƒ¼ã¨ä¸€è‡´ã™ã‚‹ã‹ç¢ºèª

**Domain knowledge:**
**ãƒ‰ãƒ¡ã‚¤ãƒ³çŸ¥è­˜:**
- Does the value make sense in real world?
  ãã®å€¤ã¯ç¾å®Ÿä¸–ç•Œã§æ„å‘³ã‚’ãªã—ã¾ã™ã‹ï¼Ÿ
- Is it plausible for this type of data?
  ã“ã®ã‚¿ã‚¤ãƒ—ã®ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦ã‚‚ã£ã¨ã‚‚ã‚‰ã—ã„ã§ã™ã‹ï¼Ÿ
- Could it be a data entry error?
  ãƒ‡ãƒ¼ã‚¿å…¥åŠ›ã‚¨ãƒ©ãƒ¼ã®å¯èƒ½æ€§ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ

---

### Impact on ML | MLã¸ã®å½±éŸ¿

**Why invalid values are problematic:**
**ãªãœç„¡åŠ¹ãªå€¤ãŒå•é¡Œãªã®ã‹:**

âŒ **Model learns from incorrect data**
   ãƒ¢ãƒ‡ãƒ«ãŒèª¤ã£ãŸãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å­¦ç¿’
   - Garbage in = Garbage out!
   - ã‚´ãƒŸã‚’å…¥ã‚Œã‚Œã°ã‚´ãƒŸãŒå‡ºã‚‹ï¼

âŒ **Skews statistics and patterns**
   çµ±è¨ˆã¨ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ­ªã‚ã‚‹
   - One huge outlier changes the average dramatically
   - 1ã¤ã®å·¨å¤§ãªå¤–ã‚Œå€¤ãŒå¹³å‡ã‚’åŠ‡çš„ã«å¤‰ãˆã‚‹

âŒ **Leads to wrong predictions**
   é–“é•ã£ãŸäºˆæ¸¬ã«ã¤ãªãŒã‚‹
   - Model thinks impossible values are normal
   - ãƒ¢ãƒ‡ãƒ«ãŒä¸å¯èƒ½ãªå€¤ã‚’æ­£å¸¸ã¨è€ƒãˆã‚‹

**Example:**
```
Company employee counts: 50, 100, 200, 150, 10000000
                                              â†‘
                                         Typo! Should be 100?

Average with error: 2,000,100 employees
Average without error: 125 employees

Model learns: "Average company has 2M employees!"
Reality: Average is actually ~125

ã‚¨ãƒ©ãƒ¼ã‚ã‚Šã®å¹³å‡ï¼š2,000,100äººã®å¾“æ¥­å“¡
ã‚¨ãƒ©ãƒ¼ãªã—ã®å¹³å‡ï¼š125äººã®å¾“æ¥­å“¡

ãƒ¢ãƒ‡ãƒ«ã¯å­¦ç¿’ï¼šã€Œå¹³å‡çš„ãªä¼æ¥­ã¯200ä¸‡äººã®å¾“æ¥­å“¡ã‚’æŒã¤ï¼ã€
ç¾å®Ÿï¼šå¹³å‡ã¯å®Ÿéš›ã«ã¯ç´„125
```

---

### Common Solutions | ä¸€èˆ¬çš„ãªè§£æ±ºç­–

| Issue | Solution | Example |
|-------|----------|---------|
| **Negative values** | Replace with null or remove | Age: -5 â†’ null |
| **Impossible values** | Remove or fix if known | Month: 13 â†’ null |
| **Extreme outliers** | Investigate, then cap or remove | Salary: $99,999,999 â†’ remove |
| **Wrong type** | Convert or remove | "Five" â†’ 5 or remove |
| **Invalid categories** | Map to valid category or "Other" | Industry: "Purple" â†’ "Other" |

| å•é¡Œ | è§£æ±ºç­– | ä¾‹ |
|------|--------|-----|
| **è² ã®å€¤** | nullã§ç½®ãæ›ãˆã‚‹ã‹å‰Šé™¤ | å¹´é½¢ï¼š-5 â†’ null |
| **ä¸å¯èƒ½ãªå€¤** | å‰Šé™¤ã¾ãŸã¯åˆ¤æ˜ã—ã¦ã„ã‚‹å ´åˆã¯ä¿®æ­£ | æœˆï¼š13 â†’ null |
| **æ¥µç«¯ãªå¤–ã‚Œå€¤** | èª¿æŸ»ã—ã¦ã‹ã‚‰ä¸Šé™ã‚’è¨­å®šã¾ãŸã¯å‰Šé™¤ | çµ¦ä¸ï¼š$99,999,999 â†’ å‰Šé™¤ |
| **é–“é•ã£ãŸã‚¿ã‚¤ãƒ—** | å¤‰æ›ã¾ãŸã¯å‰Šé™¤ | ã€ŒFiveã€â†’ 5 ã¾ãŸã¯å‰Šé™¤ |
| **ç„¡åŠ¹ãªã‚«ãƒ†ã‚´ãƒªãƒ¼** | æœ‰åŠ¹ãªã‚«ãƒ†ã‚´ãƒªãƒ¼ã¾ãŸã¯ã€Œãã®ä»–ã€ã«ãƒãƒƒãƒ— | æ¥­ç•Œï¼šã€ŒPurpleã€â†’ã€Œãã®ä»–ã€|

**Decision process:**
**æ±ºå®šãƒ—ãƒ­ã‚»ã‚¹:**
```
1. Can you fix it with certainty?
   ç¢ºå®Ÿã«ä¿®æ­£ã§ãã¾ã™ã‹ï¼Ÿ
   YES â†’ Fix it (e.g., "10OO" â†’ "1000")
   NO â†’ Go to step 2

2. Is it a critical value?
   ãã‚Œã¯é‡è¦ãªå€¤ã§ã™ã‹ï¼Ÿ
   YES â†’ Investigate further
   NO â†’ Remove the value/row

3. Is it truly impossible?
   æœ¬å½“ã«ä¸å¯èƒ½ã§ã™ã‹ï¼Ÿ
   YES â†’ Remove
   NO â†’ Keep but flag for review
```

---

## 5. Outdated Data | å¤ã„ãƒ‡ãƒ¼ã‚¿

### What to Look For | ä½•ã‚’æ¢ã™ã‹

**Time-sensitive information that's no longer accurate:**
**ã‚‚ã¯ã‚„æ­£ç¢ºã§ã¯ãªã„æ™‚é–“ã«æ•æ„Ÿãªæƒ…å ±:**

**Old company information:**
**å¤ã„ä¼æ¥­æƒ…å ±:**
```
Company: Blockbuster
Status: Active        â† Outdated! Company closed in 2013
Revenue: $5B         â† From 2004, not current
```

**Changed attributes:**
**å¤‰æ›´ã•ã‚ŒãŸå±æ€§:**
```
Company: ABC Corp
Industry: Tech (data from 2010)
Reality: Company pivoted to Healthcare in 2020
```

**Obsolete categories:**
**æ™‚ä»£é…ã‚Œã®ã‚«ãƒ†ã‚´ãƒªãƒ¼:**
```
Job categories from 2005:
- "Webmaster"        â† Rarely used title now
- "Blackberry Support" â† Technology obsolete
```

**Timestamp issues:**
**ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã®å•é¡Œ:**
```
Last updated: 2018-01-01
Current date: 2025-11-01  â† Data is 7 years old!
```

---

### How to Identify | ç‰¹å®šæ–¹æ³•

**Check dates:**
**æ—¥ä»˜ã‚’ç¢ºèª:**
```python
# Check when data was collected
df['Updated_Date'].max()  # Most recent update
df['Updated_Date'].min()  # Oldest update

# Calculate age of data
from datetime import datetime
current_date = datetime.now()
df['Data_Age_Days'] = (current_date - df['Updated_Date']).dt.days

# Find old records
old_data = df[df['Data_Age_Days'] > 365]  # Older than 1 year
```

**Domain checks:**
**ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒã‚§ãƒƒã‚¯:**
- Are companies still in business?
  ä¼æ¥­ã¯ã¾ã å–¶æ¥­ã—ã¦ã„ã¾ã™ã‹ï¼Ÿ
- Have industries/categories changed?
  æ¥­ç•Œ/ã‚«ãƒ†ã‚´ãƒªãƒ¼ã¯å¤‰æ›´ã•ã‚Œã¾ã—ãŸã‹ï¼Ÿ
- Is the information still relevant?
  æƒ…å ±ã¯ã¾ã é–¢é€£ãŒã‚ã‚Šã¾ã™ã‹ï¼Ÿ

**Cross-reference:**
**ã‚¯ãƒ­ã‚¹ãƒªãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹:**
- Check recent sources for comparison
  æ¯”è¼ƒã®ãŸã‚ã«æœ€è¿‘ã®ã‚½ãƒ¼ã‚¹ã‚’ç¢ºèª
- Verify key facts haven't changed
  ä¸»è¦ãªäº‹å®ŸãŒå¤‰æ›´ã•ã‚Œã¦ã„ãªã„ã“ã¨ã‚’ç¢ºèª
- Look for known company changes (mergers, closures)
  æ—¢çŸ¥ã®ä¼æ¥­å¤‰æ›´ã‚’æ¢ã™ï¼ˆåˆä½µã€é–‰é–ï¼‰

---

### Impact on ML | MLã¸ã®å½±éŸ¿

**Why outdated data is problematic:**
**ãªãœå¤ã„ãƒ‡ãƒ¼ã‚¿ãŒå•é¡Œãªã®ã‹:**

âŒ **Model learns from the past, not present**
   ãƒ¢ãƒ‡ãƒ«ãŒç¾åœ¨ã§ã¯ãªãéå»ã‹ã‚‰å­¦ç¿’
   - Predictions based on old patterns that no longer apply
   - ã‚‚ã¯ã‚„é©ç”¨ã•ã‚Œãªã„å¤ã„ãƒ‘ã‚¿ãƒ¼ãƒ³ã«åŸºã¥ãäºˆæ¸¬

âŒ **Misses recent trends**
   æœ€è¿‘ã®ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’è¦‹é€ƒã™
   - Can't predict what's happening NOW
   - ä»Šèµ·ã“ã£ã¦ã„ã‚‹ã“ã¨ã‚’äºˆæ¸¬ã§ããªã„

âŒ **Makes irrelevant recommendations**
   ç„¡é–¢ä¿‚ãªæ¨è–¦ã‚’è¡Œã†
   - Recommends companies that closed
   - é–‰é–ã—ãŸä¼æ¥­ã‚’æ¨è–¦
   - Suggests obsolete jobs
   - æ™‚ä»£é…ã‚Œã®ä»•äº‹ã‚’ææ¡ˆ

**Example:**
```
Job hunting scenario:

Old data (2019): Top skill = "Flash programming"
Current reality (2025): Flash is obsolete, nobody uses it

Model trained on old data: "Learn Flash to get a job!"
Reality: Flash skills are worthless now

å¤ã„ãƒ‡ãƒ¼ã‚¿ï¼ˆ2019ï¼‰ï¼šãƒˆãƒƒãƒ—ã‚¹ã‚­ãƒ« =ã€ŒFlash ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã€
ç¾åœ¨ã®ç¾å®Ÿï¼ˆ2025ï¼‰ï¼šFlash ã¯æ™‚ä»£é…ã‚Œã€èª°ã‚‚ä½¿ç”¨ã—ã¦ã„ãªã„

å¤ã„ãƒ‡ãƒ¼ã‚¿ã§è¨“ç·´ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ï¼šã€Œä»•äº‹ã‚’å¾—ã‚‹ãŸã‚ã«Flashã‚’å­¦ã¶ï¼ã€
ç¾å®Ÿï¼šFlash ã‚¹ã‚­ãƒ«ã¯ä»Šã§ã¯ä¾¡å€¤ãŒãªã„
```

---

### Common Solutions | ä¸€èˆ¬çš„ãªè§£æ±ºç­–

**1. Update the data:**
**1. ãƒ‡ãƒ¼ã‚¿ã‚’æ›´æ–°:**
- Re-scrape from current sources
  ç¾åœ¨ã®ã‚½ãƒ¼ã‚¹ã‹ã‚‰å†ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ—
- Use APIs that provide real-time data
  ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ‡ãƒ¼ã‚¿ã‚’æä¾›ã™ã‚‹APIã‚’ä½¿ç”¨
- Verify information is current
  æƒ…å ±ãŒæœ€æ–°ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª

**2. Set data freshness rules:**
**2. ãƒ‡ãƒ¼ã‚¿é®®åº¦ãƒ«ãƒ¼ãƒ«ã‚’è¨­å®š:**
```python
# Remove data older than 1 year
cutoff_date = pd.Timestamp.now() - pd.DateOffset(years=1)
df_fresh = df[df['Updated_Date'] >= cutoff_date]

# Flag old data instead of removing
df['Is_Fresh'] = df['Updated_Date'] >= cutoff_date
```

**3. Prioritize recent data:**
**3. æœ€è¿‘ã®ãƒ‡ãƒ¼ã‚¿ã‚’å„ªå…ˆ:**
- Give more weight to recent records
  æœ€è¿‘ã®ãƒ¬ã‚³ãƒ¼ãƒ‰ã«ã‚ˆã‚Šå¤šãã®é‡ã¿ã‚’ä¸ãˆã‚‹
- Use time-based filtering
  æ™‚é–“ãƒ™ãƒ¼ã‚¹ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚’ä½¿ç”¨
- Keep historical context but prioritize new
  å±¥æ­´ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ä¿æŒã™ã‚‹ãŒæ–°ã—ã„ã‚‚ã®ã‚’å„ªå…ˆ

**Best practices:**
**ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹:**
âœ… Always note when data was collected
  ãƒ‡ãƒ¼ã‚¿ãŒã„ã¤åé›†ã•ã‚ŒãŸã‹ã‚’å¸¸ã«ãƒ¡ãƒ¢
âœ… Set expiration dates for time-sensitive data
  æ™‚é–“ã«æ•æ„Ÿãªãƒ‡ãƒ¼ã‚¿ã®æœ‰åŠ¹æœŸé™ã‚’è¨­å®š
âœ… Regular updates for changing information
  å¤‰åŒ–ã™ã‚‹æƒ…å ±ã®å®šæœŸçš„ãªæ›´æ–°
âœ… Document data vintage in your analysis
  åˆ†æã§ãƒ‡ãƒ¼ã‚¿ã®ãƒ“ãƒ³ãƒ†ãƒ¼ã‚¸ã‚’æ–‡æ›¸åŒ–

---

## Data Quality Checklist Template | ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ

**Use this before training your ML model!**
**MLãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã™ã‚‹å‰ã«ã“ã‚Œã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ï¼**

### âœ… Pre-Training Checklist | è¨“ç·´å‰ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

```
â–¡ MISSING VALUES
  æ¬ æå€¤
  â–¡ Identified missing data
    æ¬ æãƒ‡ãƒ¼ã‚¿ã‚’ç‰¹å®š
  â–¡ Calculated percentage missing per column
    åˆ—ã”ã¨ã®æ¬ æç‡ã‚’è¨ˆç®—
  â–¡ Decided on solution (delete/fill)
    è§£æ±ºç­–ã‚’æ±ºå®šï¼ˆå‰Šé™¤/åŸ‹ã‚ã‚‹ï¼‰
  â–¡ Applied solution consistently
    ä¸€è²«ã—ã¦è§£æ±ºç­–ã‚’é©ç”¨

â–¡ DUPLICATES
  é‡è¤‡
  â–¡ Checked for exact duplicates
    å®Œå…¨ãªé‡è¤‡ã‚’ç¢ºèª
  â–¡ Removed exact duplicates
    å®Œå…¨ãªé‡è¤‡ã‚’å‰Šé™¤
  â–¡ Looked for partial duplicates
    éƒ¨åˆ†çš„ãªé‡è¤‡ã‚’æ¢ã—ãŸ
  â–¡ Standardized names/formats
    åå‰/ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’æ¨™æº–åŒ–
  â–¡ Removed partial duplicates
    éƒ¨åˆ†çš„ãªé‡è¤‡ã‚’å‰Šé™¤

â–¡ INCONSISTENCIES
  ä¸æ•´åˆ
  â–¡ Checked unique values in each column
    å„åˆ—ã®ä¸€æ„ã®å€¤ã‚’ç¢ºèª
  â–¡ Standardized text (lowercase, spacing)
    ãƒ†ã‚­ã‚¹ãƒˆã‚’æ¨™æº–åŒ–ï¼ˆå°æ–‡å­—ã€ã‚¹ãƒšãƒ¼ã‚¹ï¼‰
  â–¡ Standardized formats (dates, numbers)
    ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’æ¨™æº–åŒ–ï¼ˆæ—¥ä»˜ã€æ•°å€¤ï¼‰
  â–¡ Created mapping for variations
    å¤‰ç¨®ã®ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ä½œæˆ
  â–¡ Applied standardization
    æ¨™æº–åŒ–ã‚’é©ç”¨

â–¡ INVALID VALUES
  ç„¡åŠ¹ãªå€¤
  â–¡ Checked min/max values
    æœ€å°/æœ€å¤§å€¤ã‚’ç¢ºèª
  â–¡ Identified impossible values
    ä¸å¯èƒ½ãªå€¤ã‚’ç‰¹å®š
  â–¡ Checked data types
    ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—ã‚’ç¢ºèª
  â–¡ Verified categories are valid
    ã‚«ãƒ†ã‚´ãƒªãƒ¼ãŒæœ‰åŠ¹ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª
  â–¡ Removed/fixed invalid values
    ç„¡åŠ¹ãªå€¤ã‚’å‰Šé™¤/ä¿®æ­£

â–¡ OUTDATED DATA
  å¤ã„ãƒ‡ãƒ¼ã‚¿
  â–¡ Checked data collection dates
    ãƒ‡ãƒ¼ã‚¿åé›†æ—¥ã‚’ç¢ºèª
  â–¡ Verified information is current
    æƒ…å ±ãŒæœ€æ–°ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª
  â–¡ Removed/updated old records
    å¤ã„ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’å‰Šé™¤/æ›´æ–°
  â–¡ Documented data vintage
    ãƒ‡ãƒ¼ã‚¿ã®ãƒ“ãƒ³ãƒ†ãƒ¼ã‚¸ã‚’æ–‡æ›¸åŒ–

â–¡ FINAL VALIDATION
  æœ€çµ‚æ¤œè¨¼
  â–¡ Dataset has no errors
    ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã‚¨ãƒ©ãƒ¼ãŒãªã„
  â–¡ All columns have valid data
    ã™ã¹ã¦ã®åˆ—ã«æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹
  â–¡ Data is consistent and standardized
    ãƒ‡ãƒ¼ã‚¿ãŒä¸€è²«æ€§ãŒã‚ã‚Šæ¨™æº–åŒ–ã•ã‚Œã¦ã„ã‚‹
  â–¡ Ready for ML training!
    MLè¨“ç·´ã®æº–å‚™å®Œäº†ï¼
```

---

## Quick Reference Table | ã‚¯ã‚¤ãƒƒã‚¯ãƒªãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹ãƒ†ãƒ¼ãƒ–ãƒ«

| Issue | Quick Test | Quick Fix | Critical Level |
|-------|-----------|-----------|----------------|
| **Missing** | `df.isnull().sum()` | Delete or fill | High if >20% |
| **Duplicates** | `df.duplicated().sum()` | `drop_duplicates()` | High |
| **Inconsistencies** | `df['col'].unique()` | Standardize | Medium-High |
| **Invalid** | `df.describe()` | Remove/fix | High |
| **Outdated** | Check dates | Update data | Medium |

| å•é¡Œ | ã‚¯ã‚¤ãƒƒã‚¯ãƒ†ã‚¹ãƒˆ | ã‚¯ã‚¤ãƒƒã‚¯ä¿®æ­£ | é‡è¦åº¦ |
|------|--------------|------------|--------|
| **æ¬ æ** | `df.isnull().sum()` | å‰Šé™¤ã¾ãŸã¯åŸ‹ã‚ã‚‹ | >20%ã®å ´åˆé«˜ |
| **é‡è¤‡** | `df.duplicated().sum()` | `drop_duplicates()` | é«˜ |
| **ä¸æ•´åˆ** | `df['col'].unique()` | æ¨™æº–åŒ– | ä¸­-é«˜ |
| **ç„¡åŠ¹** | `df.describe()` | å‰Šé™¤/ä¿®æ­£ | é«˜ |
| **å¤ã„** | æ—¥ä»˜ã‚’ç¢ºèª | ãƒ‡ãƒ¼ã‚¿ã‚’æ›´æ–° | ä¸­ |

---

## Remember | è¦šãˆã¦ãŠã„ã¦ãã ã•ã„

**Good data is the foundation of good ML!**
**è‰¯ã„ãƒ‡ãƒ¼ã‚¿ã¯è‰¯ã„MLã®åŸºç¤ã§ã™ï¼**

No amount of advanced algorithms can fix poor quality data.
ã©ã‚“ãªã«é«˜åº¦ãªã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚‚ä½å“è³ªãƒ‡ãƒ¼ã‚¿ã‚’ä¿®æ­£ã§ãã¾ã›ã‚“ã€‚

**Time spent on data quality = Time saved on debugging predictions**
**ãƒ‡ãƒ¼ã‚¿å“è³ªã«è²»ã‚„ã™æ™‚é–“ = äºˆæ¸¬ã®ãƒ‡ãƒãƒƒã‚°ã§ç¯€ç´„ã™ã‚‹æ™‚é–“**

---

*Created for ML-101: Week 2*
*ML-101ç”¨ã«ä½œæˆï¼šç¬¬2é€±*
