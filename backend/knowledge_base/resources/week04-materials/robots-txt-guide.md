# Robots.txt Guide
# Robots.txtã‚¬ã‚¤ãƒ‰

**Week 4 Reference Material | ç¬¬4é€±å‚è€ƒè³‡æ–™**
**Course:** ML-101 Machine Learning and Intelligence

---

## What is robots.txt? | robots.txtã¨ã¯ï¼Ÿ

**robots.txt = A text file that tells web scrapers/bots what they can and cannot access**
**robots.txt = ã‚¦ã‚§ãƒ–ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼/ãƒœãƒƒãƒˆãŒä½•ã«ã‚¢ã‚¯ã‚»ã‚¹ã§ãã¦ä½•ã«ã‚¢ã‚¯ã‚»ã‚¹ã§ããªã„ã‹ã‚’ä¼ãˆã‚‹ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«**

**Think of it as:**
**æ¬¡ã®ã‚ˆã†ã«è€ƒãˆã¦ãã ã•ã„:**
- A "rule book" for automated visitors
  è‡ªå‹•è¨ªå•è€…ã®ãŸã‚ã®ã€Œãƒ«ãƒ¼ãƒ«ãƒ–ãƒƒã‚¯ã€
- Website owner's instructions to bots
  ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆæ‰€æœ‰è€…ã®ãƒœãƒƒãƒˆã¸ã®æŒ‡ç¤º
- A polite request (not a security measure!)
  ä¸å¯§ãªãƒªã‚¯ã‚¨ã‚¹ãƒˆï¼ˆã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å¯¾ç­–ã§ã¯ã‚ã‚Šã¾ã›ã‚“ï¼ï¼‰

**Location:** Always at the root of a website
**å ´æ‰€:** å¸¸ã«ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆã®ãƒ«ãƒ¼ãƒˆ

```
https://example.com/robots.txt
https://mynavi.jp/robots.txt
https://google.com/robots.txt
```

---

## Why Does robots.txt Exist? | ãªãœrobots.txtãŒå­˜åœ¨ã™ã‚‹ã®ã‹ï¼Ÿ

### Website Owners Need to: | ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆæ‰€æœ‰è€…ã¯æ¬¡ã®ã“ã¨ãŒå¿…è¦ã§ã™:

âœ… **Protect server resources**
   ã‚µãƒ¼ãƒãƒ¼ãƒªã‚½ãƒ¼ã‚¹ã‚’ä¿è­·
   - Too many bot requests can slow down or crash a website
     ãƒœãƒƒãƒˆãƒªã‚¯ã‚¨ã‚¹ãƒˆãŒå¤šã™ãã‚‹ã¨ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆãŒé…ããªã£ãŸã‚Šã‚¯ãƒ©ãƒƒã‚·ãƒ¥ã™ã‚‹

âœ… **Control indexing**
   ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆã‚’åˆ¶å¾¡
   - Tell search engines which pages to index
     æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ã«ã©ã®ãƒšãƒ¼ã‚¸ã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã™ã‚‹ã‹ã‚’ä¼ãˆã‚‹
   - Hide private or duplicate content
     ãƒ—ãƒ©ã‚¤ãƒ™ãƒ¼ãƒˆã¾ãŸã¯é‡è¤‡ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’éè¡¨ç¤º

âœ… **Prevent scraping of sensitive data**
   æ©Ÿå¯†ãƒ‡ãƒ¼ã‚¿ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’é˜²ã
   - Personal information
     å€‹äººæƒ…å ±
   - Proprietary data
     ç‹¬å ãƒ‡ãƒ¼ã‚¿
   - Database dumps
     ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ€ãƒ³ãƒ—

âœ… **Manage bandwidth**
   å¸¯åŸŸå¹…ã‚’ç®¡ç†
   - Limit how fast bots can crawl
     ãƒœãƒƒãƒˆãŒã‚¯ãƒ­ãƒ¼ãƒ«ã§ãã‚‹é€Ÿåº¦ã‚’åˆ¶é™
   - Prevent server overload
     ã‚µãƒ¼ãƒãƒ¼ã®éè² è·ã‚’é˜²ã

---

## How to Find robots.txt | robots.txtã‚’è¦‹ã¤ã‘ã‚‹æ–¹æ³•

### Simple Formula | ã‚·ãƒ³ãƒ—ãƒ«ãªå…¬å¼

```
Website URL + /robots.txt
```

**Examples:**
**ä¾‹:**

```
Website: https://mynavi.jp
Robots.txt: https://mynavi.jp/robots.txt

Website: https://en.wikipedia.org
Robots.txt: https://en.wikipedia.org/robots.txt

Website: https://github.com
Robots.txt: https://github.com/robots.txt
```

---

### Checking if robots.txt Exists | robots.txtãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèª

**Steps:**
**æ‰‹é †:**

1. **Type the website URL + `/robots.txt` in your browser**
   ãƒ–ãƒ©ã‚¦ã‚¶ã«ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆURL + `/robots.txt`ã‚’å…¥åŠ›

2. **Press Enter**
   Enterã‚’æŠ¼ã™

3. **Two possible outcomes:**
   2ã¤ã®å¯èƒ½ãªçµæœ:**

   **A) File exists:**
   **A) ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹:**
   - You see a text file with rules
     ãƒ«ãƒ¼ãƒ«ãŒå«ã¾ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒè¡¨ç¤ºã•ã‚Œã‚‹
   - Read the rules!
     ãƒ«ãƒ¼ãƒ«ã‚’èª­ã‚€ï¼

   **B) File doesn't exist:**
   **B) ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„:**
   - You see a 404 error page
     404ã‚¨ãƒ©ãƒ¼ãƒšãƒ¼ã‚¸ãŒè¡¨ç¤ºã•ã‚Œã‚‹
   - This means: No specific restrictions (but still be respectful!)
     ã“ã‚Œã¯æ„å‘³ã™ã‚‹ï¼šç‰¹å®šã®åˆ¶é™ãªã—ï¼ˆã§ã‚‚å°Šé‡ã™ã¹ãï¼ï¼‰

---

## Reading robots.txt | robots.txtã®èª­ã¿æ–¹

### Basic Structure | åŸºæœ¬æ§‹é€ 

```
User-agent: *
Disallow: /private/
Allow: /public/
Crawl-delay: 10
```

**Line by line:**
**è¡Œã”ã¨ã«:**

| Directive | Meaning | æ„å‘³ |
|-----------|---------|------|
| `User-agent:` | Which bot these rules apply to | ã“ã‚Œã‚‰ã®ãƒ«ãƒ¼ãƒ«ãŒé©ç”¨ã•ã‚Œã‚‹ãƒœãƒƒãƒˆ |
| `Disallow:` | Paths bots cannot access | ãƒœãƒƒãƒˆãŒã‚¢ã‚¯ã‚»ã‚¹ã§ããªã„ãƒ‘ã‚¹ |
| `Allow:` | Paths bots can access (even if parent is disallowed) | ãƒœãƒƒãƒˆãŒã‚¢ã‚¯ã‚»ã‚¹ã§ãã‚‹ãƒ‘ã‚¹ï¼ˆè¦ªãŒç¦æ­¢ã•ã‚Œã¦ã„ã¦ã‚‚ï¼‰ |
| `Crawl-delay:` | Seconds to wait between requests | ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“ã®å¾…æ©Ÿç§’æ•° |
| `Sitemap:` | Location of XML sitemap | XMLã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã®å ´æ‰€ |

---

## Understanding User-agent | User-agentã®ç†è§£

### What is User-agent? | User-agentã¨ã¯ï¼Ÿ

**User-agent = Identifies which bot the rules apply to**
**User-agent = ãƒ«ãƒ¼ãƒ«ãŒé©ç”¨ã•ã‚Œã‚‹ãƒœãƒƒãƒˆã‚’è­˜åˆ¥**

**Common examples:**
**ä¸€èˆ¬çš„ãªä¾‹:**

```
User-agent: *
  â†‘
Asterisk (*) = ALL bots (including yours!)

User-agent: Googlebot
  â†‘
Rules only for Google's search crawler

User-agent: Bingbot
  â†‘
Rules only for Bing's search crawler
```

---

### Example with Multiple User-agents | è¤‡æ•°ã®User-agentã®ä¾‹

```
# Rules for all bots
User-agent: *
Disallow: /admin/
Crawl-delay: 10

# Special rules for Google
User-agent: Googlebot
Disallow: /private/
Allow: /public/

# Block specific bot
User-agent: BadBot
Disallow: /
```

**How to read:**
**èª­ã¿æ–¹:**

1. **Your scraper = User-agent: \***
   ã‚ãªãŸã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ = User-agent: \*
   - Follow the rules under `User-agent: *`
     `User-agent: *`ã®ä¸‹ã®ãƒ«ãƒ¼ãƒ«ã«å¾“ã†

2. **If no rules for `*`, look for specific bot names**
   `*`ã®ãƒ«ãƒ¼ãƒ«ãŒãªã„å ´åˆã€ç‰¹å®šã®ãƒœãƒƒãƒˆåã‚’æ¢ã™
   - Usually you'll use `*` rules
     é€šå¸¸ã¯`*`ãƒ«ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã—ã¾ã™

---

## Understanding Disallow | Disallowã®ç†è§£

### What is Disallow? | Disallowã¨ã¯ï¼Ÿ

**Disallow = Paths that bots should NOT access**
**Disallow = ãƒœãƒƒãƒˆãŒã‚¢ã‚¯ã‚»ã‚¹ã™ã¹ãã§ãªã„ãƒ‘ã‚¹**

---

### Examples | ä¾‹

**1. Disallow everything:**
**1. ã™ã¹ã¦ã‚’ç¦æ­¢:**

```
User-agent: *
Disallow: /
```

**Meaning:** Don't scrape ANY page on this website
**æ„å‘³:** ã“ã®ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆã®ã©ã®ãƒšãƒ¼ã‚¸ã‚‚ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ—ã—ãªã„ã§ãã ã•ã„

---

**2. Disallow specific directory:**
**2. ç‰¹å®šã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ç¦æ­¢:**

```
User-agent: *
Disallow: /admin/
```

**Meaning:** Don't access anything in the `/admin/` folder
**æ„å‘³:** `/admin/`ãƒ•ã‚©ãƒ«ãƒ€å†…ã®ã‚‚ã®ã«ã‚¢ã‚¯ã‚»ã‚¹ã—ãªã„ã§ãã ã•ã„

**Examples of blocked URLs:**
**ãƒ–ãƒ­ãƒƒã‚¯ã•ã‚ŒãŸURLã®ä¾‹:**
- `https://example.com/admin/`
- `https://example.com/admin/users`
- `https://example.com/admin/settings`

---

**3. Disallow multiple directories:**
**3. è¤‡æ•°ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ç¦æ­¢:**

```
User-agent: *
Disallow: /admin/
Disallow: /private/
Disallow: /temp/
```

**Meaning:** Don't access `/admin/`, `/private/`, or `/temp/`
**æ„å‘³:** `/admin/`ã€`/private/`ã€ã¾ãŸã¯`/temp/`ã«ã‚¢ã‚¯ã‚»ã‚¹ã—ãªã„ã§ãã ã•ã„

---

**4. Disallow specific file types:**
**4. ç‰¹å®šã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—ã‚’ç¦æ­¢:**

```
User-agent: *
Disallow: /*.pdf$
Disallow: /*.doc$
```

**Meaning:** Don't download PDF or DOC files
**æ„å‘³:** PDFã¾ãŸã¯DOCãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ãªã„ã§ãã ã•ã„

---

**5. Allow everything (no restrictions):**
**5. ã™ã¹ã¦ã‚’è¨±å¯ï¼ˆåˆ¶é™ãªã—ï¼‰:**

```
User-agent: *
Disallow:
```

**Meaning:** Empty `Disallow` = No restrictions
**æ„å‘³:** ç©ºã®`Disallow` = åˆ¶é™ãªã—

---

## Understanding Allow | Allowã®ç†è§£

### What is Allow? | Allowã¨ã¯ï¼Ÿ

**Allow = Exceptions to Disallow rules**
**Allow = Disallowãƒ«ãƒ¼ãƒ«ã®ä¾‹å¤–**

**Use case:** Disallow a directory but allow specific files within it
**ä½¿ç”¨ä¾‹:** ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ç¦æ­¢ã™ã‚‹ãŒã€ãã®ä¸­ã®ç‰¹å®šã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¨±å¯

---

### Example | ä¾‹

```
User-agent: *
Disallow: /private/
Allow: /private/public-data.html
```

**Meaning:**
**æ„å‘³:**
- Don't access `/private/` folder
  `/private/`ãƒ•ã‚©ãƒ«ãƒ€ã«ã‚¢ã‚¯ã‚»ã‚¹ã—ãªã„ã§ãã ã•ã„
- **EXCEPT** `/private/public-data.html` is OK
  **ãŸã ã—** `/private/public-data.html`ã¯OK

**Blocked:**
**ãƒ–ãƒ­ãƒƒã‚¯:**
- `https://example.com/private/`
- `https://example.com/private/secrets.html`

**Allowed:**
**è¨±å¯:**
- `https://example.com/private/public-data.html`

---

## Understanding Crawl-delay | Crawl-delayã®ç†è§£

### What is Crawl-delay? | Crawl-delayã¨ã¯ï¼Ÿ

**Crawl-delay = Seconds to wait between requests**
**Crawl-delay = ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“ã®å¾…æ©Ÿç§’æ•°**

**Purpose:** Prevent overwhelming the server with too many requests too fast
**ç›®çš„:** é€Ÿã™ãã‚‹å¤šæ•°ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã§ã‚µãƒ¼ãƒãƒ¼ã‚’åœ§å€’ã—ãªã„ã‚ˆã†ã«ã™ã‚‹

---

### Examples | ä¾‹

```
User-agent: *
Crawl-delay: 10
```

**Meaning:** Wait 10 seconds between each request
**æ„å‘³:** å„ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®é–“ã«10ç§’å¾…ã¤

**In practice:**
**å®Ÿéš›ã«ã¯:**
```
Request page 1 â†’ Wait 10 seconds â†’ Request page 2 â†’ Wait 10 seconds â†’ Request page 3
```

---

**Common crawl-delay values:**
**ä¸€èˆ¬çš„ãªcrawl-delayå€¤:**

| Value | Meaning | When Used |
|-------|---------|-----------|
| `1` | 1 second delay | Light restriction |
| `5` | 5 second delay | Moderate restriction |
| `10` | 10 second delay | Heavy restriction |
| `30+` | 30+ second delay | Very heavy restriction |

| å€¤ | æ„å‘³ | ã„ã¤ä½¿ç”¨ã•ã‚Œã‚‹ã‹ |
|----|------|----------------|
| `1` | 1ç§’é…å»¶ | è»½ã„åˆ¶é™ |
| `5` | 5ç§’é…å»¶ | ä¸­ç¨‹åº¦ã®åˆ¶é™ |
| `10` | 10ç§’é…å»¶ | é‡ã„åˆ¶é™ |
| `30+` | 30ç§’ä»¥ä¸Šé…å»¶ | éå¸¸ã«é‡ã„åˆ¶é™ |

---

### Implementing Crawl-delay | Crawl-delayã®å®Ÿè£…

**In Python:**
```python
import time
import requests

crawl_delay = 10  # From robots.txt

# Request first page
response = requests.get('https://example.com/page1')

# Wait before next request
time.sleep(crawl_delay)

# Request second page
response = requests.get('https://example.com/page2')
```

**Always respect crawl-delay!**
**å¸¸ã«crawl-delayã‚’å°Šé‡ã—ã¦ãã ã•ã„ï¼**

---

## Real-World Examples | å®Ÿä¸–ç•Œã®ä¾‹

### Example 1: Wikipedia | ä¾‹1ï¼šWikipedia

**File:** `https://en.wikipedia.org/robots.txt`

```
User-agent: *
Disallow: /wiki/Special:
Disallow: /wiki/User:
Disallow: /wiki/Wikipedia:
Allow: /wiki/
Crawl-delay: 1
```

**What this means:**
**ã“ã‚ŒãŒæ„å‘³ã™ã‚‹ã“ã¨:**
- âœ… Can scrape regular articles (`/wiki/Article_Name`)
  é€šå¸¸ã®è¨˜äº‹ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ—ã§ãã‚‹ï¼ˆ`/wiki/Article_Name`ï¼‰
- âŒ Cannot scrape special pages (`/wiki/Special:`)
  ç‰¹åˆ¥ãƒšãƒ¼ã‚¸ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ—ã§ããªã„ï¼ˆ`/wiki/Special:`ï¼‰
- âŒ Cannot scrape user pages (`/wiki/User:`)
  ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒšãƒ¼ã‚¸ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ—ã§ããªã„ï¼ˆ`/wiki/User:`ï¼‰
- â± Wait 1 second between requests
  ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“ã«1ç§’å¾…ã¤

---

### Example 2: Restrictive Site | ä¾‹2ï¼šåˆ¶é™çš„ãªã‚µã‚¤ãƒˆ

```
User-agent: *
Disallow: /
```

**What this means:**
**ã“ã‚ŒãŒæ„å‘³ã™ã‚‹ã“ã¨:**
- âŒ **NO scraping allowed at all**
  **ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã¯ä¸€åˆ‡è¨±å¯ã•ã‚Œã¦ã„ã¾ã›ã‚“**
- Website owner doesn't want ANY automated access
  ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆæ‰€æœ‰è€…ã¯è‡ªå‹•ã‚¢ã‚¯ã‚»ã‚¹ã‚’ä¸€åˆ‡æœ›ã‚“ã§ã„ã¾ã›ã‚“

**What to do:**
**ä½•ã‚’ã™ã¹ãã‹:**
- **Respect this!** Don't scrape
  **ã“ã‚Œã‚’å°Šé‡ã—ã¦ãã ã•ã„ï¼** ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ—ã—ãªã„ã§ãã ã•ã„
- Contact website owner for permission
  è¨±å¯ã®ãŸã‚ã«ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆæ‰€æœ‰è€…ã«é€£çµ¡
- Look for official API instead
  ä»£ã‚ã‚Šã«å…¬å¼APIã‚’æ¢ã™

---

### Example 3: Permissive Site | ä¾‹3ï¼šè¨±å¯çš„ãªã‚µã‚¤ãƒˆ

```
User-agent: *
Disallow: /admin/
Disallow: /private/
Crawl-delay: 5

Sitemap: https://example.com/sitemap.xml
```

**What this means:**
**ã“ã‚ŒãŒæ„å‘³ã™ã‚‹ã“ã¨:**
- âœ… Can scrape most pages
  ã»ã¨ã‚“ã©ã®ãƒšãƒ¼ã‚¸ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ—ã§ãã‚‹
- âŒ Except `/admin/` and `/private/`
  `/admin/`ã¨`/private/`ã‚’é™¤ã
- â± Wait 5 seconds between requests
  ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“ã«5ç§’å¾…ã¤
- ğŸ“„ Sitemap available for finding pages
  ãƒšãƒ¼ã‚¸ã‚’è¦‹ã¤ã‘ã‚‹ãŸã‚ã®ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ãŒåˆ©ç”¨å¯èƒ½

---

### Example 4: Different Rules for Different Bots | ä¾‹4ï¼šç•°ãªã‚‹ãƒœãƒƒãƒˆç”¨ã®ç•°ãªã‚‹ãƒ«ãƒ¼ãƒ«

```
# Rules for most bots
User-agent: *
Disallow: /admin/
Crawl-delay: 10

# More permissive for Google
User-agent: Googlebot
Disallow: /admin/
Crawl-delay: 1

# Block specific aggressive bot
User-agent: AggresiveBot
Disallow: /
```

**What this means:**
**ã“ã‚ŒãŒæ„å‘³ã™ã‚‹ã“ã¨:**
- Your scraper follows `User-agent: *` rules
  ã‚ãªãŸã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã¯`User-agent: *`ãƒ«ãƒ¼ãƒ«ã«å¾“ã„ã¾ã™
- Google's bot has faster crawl-delay
  Googleã®ãƒœãƒƒãƒˆã¯ã‚ˆã‚Šé€Ÿã„crawl-delayã‚’æŒã¡ã¾ã™
- "AggresiveBot" is completely blocked
  ã€ŒAggresiveBotã€ã¯å®Œå…¨ã«ãƒ–ãƒ­ãƒƒã‚¯ã•ã‚Œã¦ã„ã¾ã™

---

## What to Do If Scraping is Disallowed | ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãŒç¦æ­¢ã•ã‚Œã¦ã„ã‚‹å ´åˆã®å¯¾å‡¦æ³•

### Scenario: robots.txt says `Disallow: /` | ã‚·ãƒŠãƒªã‚ªï¼šrobots.txtãŒ`Disallow: /`ã¨è¨€ã£ã¦ã„ã‚‹

**You have several options:**
**ã„ãã¤ã‹ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ãŒã‚ã‚Šã¾ã™:**

---

### Option 1: Respect It (Recommended) | ã‚ªãƒ—ã‚·ãƒ§ãƒ³1ï¼šãã‚Œã‚’å°Šé‡ã™ã‚‹ï¼ˆæ¨å¥¨ï¼‰

**Don't scrape the website**
**ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ—ã—ãªã„ã§ãã ã•ã„**

**Why:**
**ãªãœ:**
- âœ… Ethical
  å€«ç†çš„
- âœ… Legal (in many jurisdictions)
  æ³•çš„ï¼ˆå¤šãã®ç®¡è½„åŒºåŸŸã§ï¼‰
- âœ… Respectful to website owner
  ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆæ‰€æœ‰è€…ã«å¯¾ã—ã¦æ•¬æ„ã‚’æ‰•ã†

---

### Option 2: Contact Website Owner | ã‚ªãƒ—ã‚·ãƒ§ãƒ³2ï¼šã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆæ‰€æœ‰è€…ã«é€£çµ¡

**Ask for permission**
**è¨±å¯ã‚’æ±‚ã‚ã‚‹**

**Email template:**
**ãƒ¡ãƒ¼ãƒ«ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ:**

```
Subject: Request for Permission to Scrape Data for Educational Project
ä»¶åï¼šæ•™è‚²ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãŸã‚ã®ãƒ‡ãƒ¼ã‚¿ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°è¨±å¯ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆ

Dear [Website Name] Team,
è¦ªæ„›ãªã‚‹[ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆå]ãƒãƒ¼ãƒ ã€

I am a student working on a machine learning project at [University Name].
I would like to collect [specific data] from your website for educational purposes only.

The data will:
- Be used only for learning and non-commercial purposes
- Not be shared publicly
- Be collected at a respectful rate (no server overload)

Could you please grant permission or suggest an alternative data source?

Thank you for your consideration.

Best regards,
[Your Name]
[Your Email]
```

**If they say yes:**
**å½¼ã‚‰ãŒã‚¤ã‚¨ã‚¹ã¨è¨€ã£ãŸå ´åˆ:**
- âœ… Get written permission
  æ›¸é¢ã«ã‚ˆã‚‹è¨±å¯ã‚’å¾—ã‚‹
- âœ… Follow any conditions they specify
  å½¼ã‚‰ãŒæŒ‡å®šã™ã‚‹æ¡ä»¶ã«å¾“ã†

**If they say no:**
**å½¼ã‚‰ãŒãƒãƒ¼ã¨è¨€ã£ãŸå ´åˆ:**
- âŒ Don't scrape
  ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ—ã—ãªã„ã§ãã ã•ã„
- Move to Option 3 or 4
  ã‚ªãƒ—ã‚·ãƒ§ãƒ³3ã¾ãŸã¯4ã«ç§»å‹•

---

### Option 3: Look for Official API | ã‚ªãƒ—ã‚·ãƒ§ãƒ³3ï¼šå…¬å¼APIã‚’æ¢ã™

**Many websites offer APIs (Application Programming Interfaces)**
**å¤šãã®ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆãŒAPIï¼ˆã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ï¼‰ã‚’æä¾›**

**Advantages:**
**åˆ©ç‚¹:**
- âœ… Legal and approved
  æ³•çš„ã§æ‰¿èªæ¸ˆã¿
- âœ… Structured data (easier to use)
  æ§‹é€ åŒ–ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ï¼ˆä½¿ã„ã‚„ã™ã„ï¼‰
- âœ… More reliable
  ã‚ˆã‚Šä¿¡é ¼æ€§ãŒé«˜ã„

**How to find:**
**è¦‹ã¤ã‘ã‚‹æ–¹æ³•:**
- Look for "API", "Developers", or "Data" in website footer
  ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆãƒ•ãƒƒã‚¿ãƒ¼ã§ã€ŒAPIã€ã€ã€Œé–‹ç™ºè€…ã€ã€ã¾ãŸã¯ã€Œãƒ‡ãƒ¼ã‚¿ã€ã‚’æ¢ã™
- Search: "[website name] API"
  æ¤œç´¢ï¼šã€Œ[ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆå] APIã€
- Check documentation
  ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ç¢ºèª

**Examples:**
**ä¾‹:**
- Twitter API
- GitHub API
- Weather API

---

### Option 4: Use Alternative Data Sources | ã‚ªãƒ—ã‚·ãƒ§ãƒ³4ï¼šä»£æ›¿ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‚’ä½¿ç”¨

**Find similar data elsewhere:**
**ä»–ã®å ´æ‰€ã§é¡ä¼¼ã®ãƒ‡ãƒ¼ã‚¿ã‚’è¦‹ã¤ã‘ã‚‹:**

âœ… Open datasets (Kaggle, UCI ML Repository, data.gov)
   ã‚ªãƒ¼ãƒ—ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆKaggleã€UCI MLãƒªãƒã‚¸ãƒˆãƒªã€data.govï¼‰

âœ… Public databases
   å…¬é–‹ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹

âœ… Government open data portals
   æ”¿åºœã®ã‚ªãƒ¼ãƒ—ãƒ³ãƒ‡ãƒ¼ã‚¿ãƒãƒ¼ã‚¿ãƒ«

âœ… Academic datasets
   å­¦è¡“ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ

âœ… Data provided by your instructor
   ã‚¤ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚¿ãƒ¼ãŒæä¾›ã™ã‚‹ãƒ‡ãƒ¼ã‚¿

---

### Option 5: Manual Collection (Small Scale) | ã‚ªãƒ—ã‚·ãƒ§ãƒ³5ï¼šæ‰‹å‹•åé›†ï¼ˆå°è¦æ¨¡ï¼‰

**If you only need small amount of data:**
**å°‘é‡ã®ãƒ‡ãƒ¼ã‚¿ã®ã¿ãŒå¿…è¦ãªå ´åˆ:**

- Manually copy-paste information
  æƒ…å ±ã‚’æ‰‹å‹•ã§ã‚³ãƒ”ãƒ¼ï¼†ãƒšãƒ¼ã‚¹ãƒˆ
- Take screenshots
  ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã‚’æ’®ã‚‹
- Type data into spreadsheet
  ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«ãƒ‡ãƒ¼ã‚¿ã‚’å…¥åŠ›

**Appropriate for:**
**é©åˆ‡ãªå ´åˆ:**
- âœ… Small datasets (<100 items)
  å°è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆ<100é …ç›®ï¼‰
- âœ… Learning exercises
  å­¦ç¿’æ¼”ç¿’
- âœ… Proof of concept
  æ¦‚å¿µå®Ÿè¨¼

**Not appropriate for:**
**é©åˆ‡ã§ãªã„å ´åˆ:**
- âŒ Large datasets (1000s of items)
  å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆ1000é …ç›®ï¼‰
- âŒ Production systems
  æœ¬ç•ªã‚·ã‚¹ãƒ†ãƒ 

---

## Best Practices | ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

### âœ… Always Check robots.txt First | å¸¸ã«æœ€åˆã«robots.txtã‚’ç¢ºèª

**Before scraping ANY website:**
**ä»»æ„ã®ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ—ã™ã‚‹å‰ã«:**

1. Go to `website.com/robots.txt`
   `website.com/robots.txt`ã«ç§»å‹•
2. Read the rules
   ãƒ«ãƒ¼ãƒ«ã‚’èª­ã‚€
3. Follow the rules
   ãƒ«ãƒ¼ãƒ«ã«å¾“ã†

---

### âœ… Respect Crawl-delay | Crawl-delayã‚’å°Šé‡

**If robots.txt specifies crawl-delay:**
**robots.txtãŒcrawl-delayã‚’æŒ‡å®šã—ã¦ã„ã‚‹å ´åˆ:**

```python
import time

crawl_delay = 10  # From robots.txt

for url in urls:
    response = requests.get(url)
    # Process response
    time.sleep(crawl_delay)  # IMPORTANT!
```

**Even if no crawl-delay specified:**
**crawl-delayãŒæŒ‡å®šã•ã‚Œã¦ã„ãªãã¦ã‚‚:**
- Add small delay (1-2 seconds)
  å°ã•ãªé…å»¶ã‚’è¿½åŠ ï¼ˆ1-2ç§’ï¼‰
- Be polite to the server
  ã‚µãƒ¼ãƒãƒ¼ã«ç¤¼å„€æ­£ã—ã

---

### âœ… Use Descriptive User-agent | èª¬æ˜çš„ãªUser-agentã‚’ä½¿ç”¨

**Identify your scraper:**
**ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã‚’è­˜åˆ¥:**

```python
headers = {
    'User-Agent': 'StudentBot/1.0 (ML-101 Educational Project; contact@email.com)'
}

response = requests.get(url, headers=headers)
```

**Why:**
**ãªãœ:**
- Transparent about who you are
  ã‚ãªãŸãŒèª°ã§ã‚ã‚‹ã‹ã«ã¤ã„ã¦é€æ˜
- Website owner can contact you if needed
  å¿…è¦ã«å¿œã˜ã¦ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆæ‰€æœ‰è€…ãŒé€£çµ¡ã§ãã‚‹
- Professional practice
  ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«ãªå®Ÿè·µ

---

### âœ… Document Your Compliance | ã‚³ãƒ³ãƒ—ãƒ©ã‚¤ã‚¢ãƒ³ã‚¹ã‚’æ–‡æ›¸åŒ–

**Keep notes:**
**ãƒ¡ãƒ¢ã‚’ä¿ç®¡:**

```
Project: Company Data Collection
Website: https://example.com
Robots.txt: https://example.com/robots.txt
Date checked: 2025-11-01

Relevant rules:
- User-agent: *
- Disallow: /admin/
- Crawl-delay: 5

Compliance:
- Not accessing /admin/
- Implementing 5-second delay
- Using custom User-Agent string
```

---

### âœ… When in Doubt, Ask | ç–‘å•ãŒã‚ã‚Œã°å°‹ã­ã‚‹

**If robots.txt is unclear:**
**robots.txtãŒä¸æ˜ç¢ºãªå ´åˆ:**
- Contact website owner
  ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆæ‰€æœ‰è€…ã«é€£çµ¡
- Ask instructor
  ã‚¤ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚¿ãƒ¼ã«å°‹ã­ã‚‹
- Choose more restrictive interpretation
  ã‚ˆã‚Šåˆ¶é™çš„ãªè§£é‡ˆã‚’é¸æŠ

**Better safe than sorry!**
**å®‰å…¨ç¬¬ä¸€ï¼**

---

## Common Misconceptions | ä¸€èˆ¬çš„ãªèª¤è§£

### âŒ Myth 1: "robots.txt is a security measure" | èª¤è§£1ï¼šã€Œrobots.txtã¯ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å¯¾ç­–ã€

**Reality:**
**ç¾å®Ÿ:**
- robots.txt is a REQUEST, not enforcement
  robots.txtã¯ãƒªã‚¯ã‚¨ã‚¹ãƒˆã§ã‚ã‚Šã€å¼·åˆ¶ã§ã¯ã‚ã‚Šã¾ã›ã‚“
- Anyone can ignore it (but shouldn't!)
  èª°ã§ã‚‚ç„¡è¦–ã§ãã‚‹ï¼ˆã§ã‚‚ã™ã¹ãã§ã¯ã‚ã‚Šã¾ã›ã‚“ï¼ï¼‰
- Not a substitute for actual security
  å®Ÿéš›ã®ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã®ä»£æ›¿ã§ã¯ã‚ã‚Šã¾ã›ã‚“

---

### âŒ Myth 2: "If there's no robots.txt, I can scrape anything" | èª¤è§£2ï¼šã€Œrobots.txtãŒãªã‘ã‚Œã°ä½•ã§ã‚‚ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ—ã§ãã‚‹ã€

**Reality:**
**ç¾å®Ÿ:**
- Absence of robots.txt doesn't mean unlimited access
  robots.txtã®ä¸åœ¨ã¯ç„¡åˆ¶é™ã®ã‚¢ã‚¯ã‚»ã‚¹ã‚’æ„å‘³ã—ã¾ã›ã‚“
- Still need to respect:
  ã¾ã å°Šé‡ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ï¼š
  - Terms of Service
    åˆ©ç”¨è¦ç´„
  - Copyright laws
    è‘—ä½œæ¨©æ³•
  - Data protection laws
    ãƒ‡ãƒ¼ã‚¿ä¿è­·æ³•
  - Server resources
    ã‚µãƒ¼ãƒãƒ¼ãƒªã‚½ãƒ¼ã‚¹

---

### âŒ Myth 3: "Crawl-delay is optional" | èª¤è§£3ï¼šã€ŒCrawl-delayã¯ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã€

**Reality:**
**ç¾å®Ÿ:**
- Crawl-delay is an explicit request
  Crawl-delayã¯æ˜ç¤ºçš„ãªãƒªã‚¯ã‚¨ã‚¹ãƒˆã§ã™
- Ignoring it can:
  ç„¡è¦–ã™ã‚‹ã¨æ¬¡ã®ã“ã¨ãŒã§ãã¾ã™ï¼š
  - Overload servers
    ã‚µãƒ¼ãƒãƒ¼ã‚’éè² è·ã«ã™ã‚‹
  - Get your IP blocked
    IPã‚’ãƒ–ãƒ­ãƒƒã‚¯ã•ã‚Œã‚‹
  - Violate ethical practices
    å€«ç†çš„æ…£è¡Œã«é•åã™ã‚‹

---

### âŒ Myth 4: "I'm just a student, rules don't apply to me" | èª¤è§£4ï¼šã€Œç§ã¯ãŸã ã®å­¦ç”Ÿã€ãƒ«ãƒ¼ãƒ«ã¯ç§ã«é©ç”¨ã•ã‚Œãªã„ã€

**Reality:**
**ç¾å®Ÿ:**
- Rules apply to EVERYONE
  ãƒ«ãƒ¼ãƒ«ã¯ã™ã¹ã¦ã®äººã«é©ç”¨ã•ã‚Œã¾ã™
- Being a student is not an excuse
  å­¦ç”Ÿã§ã‚ã‚‹ã“ã¨ã¯è¨€ã„è¨³ã«ãªã‚Šã¾ã›ã‚“
- Use this as opportunity to learn ethical practices!
  ã“ã‚Œã‚’å€«ç†çš„æ…£è¡Œã‚’å­¦ã¶æ©Ÿä¼šã¨ã—ã¦ä½¿ç”¨ã—ã¦ãã ã•ã„ï¼

---

## Quick Reference Card | ã‚¯ã‚¤ãƒƒã‚¯ãƒªãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹ã‚«ãƒ¼ãƒ‰

### How to Check robots.txt | robots.txtã®ç¢ºèªæ–¹æ³•

```
1. Go to: website.com/robots.txt
2. Read the rules
3. Follow them!
```

---

### Key Directives | ä¸»è¦ãªãƒ‡ã‚£ãƒ¬ã‚¯ãƒ†ã‚£ãƒ–

| Directive | Meaning | Action |
|-----------|---------|--------|
| `User-agent: *` | Applies to all bots | That's you! |
| `Disallow: /path/` | Don't access this path | Avoid it |
| `Allow: /path/` | OK to access | You can scrape |
| `Crawl-delay: 10` | Wait 10 seconds | Add delay in code |

| ãƒ‡ã‚£ãƒ¬ã‚¯ãƒ†ã‚£ãƒ– | æ„å‘³ | ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ |
|-------------|------|----------|
| `User-agent: *` | ã™ã¹ã¦ã®ãƒœãƒƒãƒˆã«é©ç”¨ | ãã‚Œã¯ã‚ãªãŸã§ã™ï¼ |
| `Disallow: /path/` | ã“ã®ãƒ‘ã‚¹ã«ã‚¢ã‚¯ã‚»ã‚¹ã—ãªã„ | é¿ã‘ã‚‹ |
| `Allow: /path/` | ã‚¢ã‚¯ã‚»ã‚¹OK | ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ—ã§ãã¾ã™ |
| `Crawl-delay: 10` | 10ç§’å¾…ã¤ | ã‚³ãƒ¼ãƒ‰ã«é…å»¶ã‚’è¿½åŠ  |

---

### Decision Flowchart | æ±ºå®šãƒ•ãƒ­ãƒ¼ãƒãƒ£ãƒ¼ãƒˆ

```
Can I scrape this website?
ã“ã®ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ—ã§ãã¾ã™ã‹ï¼Ÿ

â†“

Check robots.txt
robots.txtã‚’ç¢ºèª

â†“

Disallow: / ?
   YES â†’ Don't scrape (or ask permission)
          ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ—ã—ãªã„ï¼ˆã¾ãŸã¯è¨±å¯ã‚’æ±‚ã‚ã‚‹ï¼‰
   NO â†’ Continue
         ç¶šã‘ã‚‹

â†“

Is the data I want disallowed?
æ¬²ã—ã„ãƒ‡ãƒ¼ã‚¿ã¯ç¦æ­¢ã•ã‚Œã¦ã„ã¾ã™ã‹ï¼Ÿ
   YES â†’ Don't scrape that section
          ãã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ—ã—ãªã„
   NO â†’ Continue
         ç¶šã‘ã‚‹

â†“

Is there a crawl-delay?
crawl-delayã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ
   YES â†’ Implement delay in code
          ã‚³ãƒ¼ãƒ‰ã«é…å»¶ã‚’å®Ÿè£…
   NO â†’ Still add small delay (1-2 sec)
         ãã‚Œã§ã‚‚å°ã•ãªé…å»¶ã‚’è¿½åŠ ï¼ˆ1-2ç§’ï¼‰

â†“

âœ… You can scrape (ethically)!
   ï¼ˆå€«ç†çš„ã«ï¼‰ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ—ã§ãã¾ã™ï¼
```

---

## Practice Exercise | ç·´ç¿’å•é¡Œ

**Given this robots.txt:**
**ã“ã®robots.txtãŒä¸ãˆã‚‰ã‚ŒãŸå ´åˆ:**

```
User-agent: *
Disallow: /admin/
Disallow: /user/private/
Allow: /user/public/
Crawl-delay: 5

User-agent: Googlebot
Disallow: /admin/
Crawl-delay: 1

Sitemap: https://example.com/sitemap.xml
```

**Questions:**
**è³ªå•:**

1. Can you scrape `https://example.com/products/` ?
   `https://example.com/products/`ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ—ã§ãã¾ã™ã‹ï¼Ÿ
   **Answer:** Yes (not disallowed)
   **ç­”ãˆ:** ã¯ã„ï¼ˆç¦æ­¢ã•ã‚Œã¦ã„ã¾ã›ã‚“ï¼‰

2. Can you scrape `https://example.com/admin/users` ?
   `https://example.com/admin/users`ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ—ã§ãã¾ã™ã‹ï¼Ÿ
   **Answer:** No (disallowed)
   **ç­”ãˆ:** ã„ã„ãˆï¼ˆç¦æ­¢ï¼‰

3. Can you scrape `https://example.com/user/public/profile` ?
   `https://example.com/user/public/profile`ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ—ã§ãã¾ã™ã‹ï¼Ÿ
   **Answer:** Yes (explicitly allowed)
   **ç­”ãˆ:** ã¯ã„ï¼ˆæ˜ç¤ºçš„ã«è¨±å¯ï¼‰

4. How long should you wait between requests?
   ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“ã«ã©ã‚Œãã‚‰ã„å¾…ã¤ã¹ãã§ã™ã‹ï¼Ÿ
   **Answer:** 5 seconds (crawl-delay for User-agent: *)
   **ç­”ãˆ:** 5ç§’ï¼ˆUser-agent: *ã®crawl-delayï¼‰

5. Where can you find a list of pages to scrape?
   ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ—ã™ã‚‹ãƒšãƒ¼ã‚¸ã®ãƒªã‚¹ãƒˆã¯ã©ã“ã§è¦‹ã¤ã‘ã‚‰ã‚Œã¾ã™ã‹ï¼Ÿ
   **Answer:** https://example.com/sitemap.xml
   **ç­”ãˆ:** https://example.com/sitemap.xml

---

*Created for ML-101: Week 4*
*ML-101ç”¨ã«ä½œæˆï¼šç¬¬4é€±*
