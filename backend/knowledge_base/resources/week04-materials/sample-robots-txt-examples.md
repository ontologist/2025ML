# robots.txt Examples and Analysis Guide
# robots.txt ã®å®Ÿä¾‹ã¨åˆ†æã‚¬ã‚¤ãƒ‰

**Week 4 Web Scraping Training Material**
**Week 4 Webã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å­¦ç¿’æ•™æ**

---

## Introduction | ã¯ã˜ã‚ã«

The `robots.txt` file is a text file placed in the root directory of a website that tells web crawlers and scraping bots which pages they are allowed or not allowed to access. Before scraping any website, you should **always check the robots.txt file** to ensure you're complying with the site's policies.

`robots.txt`ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã€Webã‚µã‚¤ãƒˆã®ãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«é…ç½®ã•ã‚Œã‚‹ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã§ã€Webã‚¯ãƒ­ãƒ¼ãƒ©ãƒ¼ã‚„ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒœãƒƒãƒˆã«å¯¾ã—ã¦ã€ã‚¢ã‚¯ã‚»ã‚¹ãŒè¨±å¯ã•ã‚Œã¦ã„ã‚‹ãƒšãƒ¼ã‚¸ã¨ç¦æ­¢ã•ã‚Œã¦ã„ã‚‹ãƒšãƒ¼ã‚¸ã‚’æŒ‡ç¤ºã—ã¾ã™ã€‚ã©ã®Webã‚µã‚¤ãƒˆã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã™ã‚‹å‰ã«ã‚‚ã€**å¿…ãšrobots.txtãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèª**ã—ã€ã‚µã‚¤ãƒˆã®ãƒãƒªã‚·ãƒ¼ã«å¾“ã†ã‚ˆã†ã«ã—ã¦ãã ã•ã„ã€‚

### How to Access robots.txt | robots.txtã¸ã®ã‚¢ã‚¯ã‚»ã‚¹æ–¹æ³•

Simply append `/robots.txt` to the domain:
- `https://www.example.com/robots.txt`
- `https://www.mynavi.jp/robots.txt`
- `https://www.linkedin.com/robots.txt`

ãƒ‰ãƒ¡ã‚¤ãƒ³ã«`/robots.txt`ã‚’è¿½åŠ ã™ã‚‹ã ã‘ã§ã™ã€‚

---

## Example 1: Mynavi.jp (Japanese Job Portal)
## ä¾‹1: ãƒã‚¤ãƒŠãƒ“ï¼ˆæ—¥æœ¬ã®æ±‚äººãƒãƒ¼ã‚¿ãƒ«ï¼‰

**URL:** `https://www.mynavi.jp/robots.txt`

### robots.txt Content:
```
User-agent: *
Disallow: /admin/
Disallow: /api/
Disallow: /private/
Disallow: /cgi-bin/
Disallow: /search?
Allow: /company/
Allow: /job/
Crawl-delay: 10

User-agent: Googlebot
Disallow: /admin/
Allow: /

User-agent: CCBot
Disallow: /
```

### Explanation | è§£èª¬:

**User-agent: \*** - Applies to all bots
**User-agent: \*** - ã™ã¹ã¦ã®ãƒœãƒƒãƒˆã«é©ç”¨

- âŒ **Disallow: /admin/** - Admin pages are forbidden | ç®¡ç†ãƒšãƒ¼ã‚¸ã¯ç¦æ­¢
- âŒ **Disallow: /api/** - API endpoints are forbidden | APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã¯ç¦æ­¢
- âŒ **Disallow: /private/** - Private directories forbidden | ãƒ—ãƒ©ã‚¤ãƒ™ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªç¦æ­¢
- âŒ **Disallow: /search?** - Search pages with parameters forbidden | ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ä»˜ãæ¤œç´¢ãƒšãƒ¼ã‚¸ã¯ç¦æ­¢
- âœ… **Allow: /company/** - Company pages are allowed | ä¼æ¥­ãƒšãƒ¼ã‚¸ã¯è¨±å¯
- âœ… **Allow: /job/** - Job listing pages are allowed | æ±‚äººãƒšãƒ¼ã‚¸ã¯è¨±å¯
- â±ï¸ **Crawl-delay: 10** - Wait 10 seconds between requests | ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“ã«10ç§’å¾…æ©Ÿ

**User-agent: Googlebot** - Special rules for Google's crawler
- âŒ **Disallow: /admin/** - Still can't access admin | ç®¡ç†ãƒšãƒ¼ã‚¸ã¯ã‚¢ã‚¯ã‚»ã‚¹ä¸å¯
- âœ… **Allow: /** - Everything else is allowed | ãã®ä»–ã¯ã™ã¹ã¦è¨±å¯

**User-agent: CCBot** - Common Crawl bot
- âŒ **Disallow: /** - Completely blocked | å®Œå…¨ã«ãƒ–ãƒ­ãƒƒã‚¯

### Interpretation for Scraping | ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã«ãŠã‘ã‚‹è§£é‡ˆ:

- âœ… **You CAN scrape:** Job listings (/job/) and company pages (/company/)
- âŒ **You CANNOT scrape:** Admin pages, API endpoints, search results
- âš ï¸ **Rate limiting:** Must wait 10 seconds between requests
- ğŸ’¡ **Best practice:** Follow the crawl-delay to avoid being blocked

---

## Example 2: Indeed.com (Global Job Portal)
## ä¾‹2: Indeedï¼ˆã‚°ãƒ­ãƒ¼ãƒãƒ«æ±‚äººãƒãƒ¼ã‚¿ãƒ«ï¼‰

**URL:** `https://www.indeed.com/robots.txt`

### robots.txt Content:
```
User-agent: *
Disallow: /rc/
Disallow: /rpc/
Disallow: /preferences/
Crawl-delay: 5
Request-rate: 1/5

User-agent: Googlebot
Allow: /

User-agent: bingbot
Allow: /

User-agent: GPTBot
Disallow: /

User-agent: ChatGPT-User
Disallow: /
```

### Explanation | è§£èª¬:

**User-agent: \*** - General rules for all bots
- âŒ **Disallow: /rc/** - Remote call endpoints forbidden | ãƒªãƒ¢ãƒ¼ãƒˆã‚³ãƒ¼ãƒ«ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆç¦æ­¢
- âŒ **Disallow: /rpc/** - RPC endpoints forbidden | RPCã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆç¦æ­¢
- âŒ **Disallow: /preferences/** - User preference pages forbidden | ãƒ¦ãƒ¼ã‚¶ãƒ¼è¨­å®šãƒšãƒ¼ã‚¸ç¦æ­¢
- â±ï¸ **Crawl-delay: 5** - 5 second delay between requests | ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“5ç§’ã®é…å»¶
- â±ï¸ **Request-rate: 1/5** - Maximum 1 request every 5 seconds | 5ç§’ã”ã¨ã«æœ€å¤§1ãƒªã‚¯ã‚¨ã‚¹ãƒˆ

**Special Bots:**
- âœ… **Googlebot & Bingbot:** Full access allowed | å®Œå…¨ã‚¢ã‚¯ã‚»ã‚¹è¨±å¯
- âŒ **GPTBot & ChatGPT-User:** Completely blocked (AI training bots) | å®Œå…¨ãƒ–ãƒ­ãƒƒã‚¯ï¼ˆAIå­¦ç¿’ãƒœãƒƒãƒˆï¼‰

### Interpretation for Scraping | ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã«ãŠã‘ã‚‹è§£é‡ˆ:

- âœ… **You CAN scrape:** Most job listings and public pages
- âŒ **You CANNOT scrape:** Backend API endpoints, user settings
- âš ï¸ **Rate limiting:** Strict 5-second delay required
- ğŸ’¡ **Note:** AI training bots are explicitly blocked

---

## Example 3: LinkedIn.com (Professional Network)
## ä¾‹3: LinkedInï¼ˆãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼‰

**URL:** `https://www.linkedin.com/robots.txt`

### robots.txt Content:
```
User-agent: *
Disallow: /

User-agent: LinkedInBot
Allow: /

User-agent: Googlebot
Allow: /$
Allow: /company/*
Allow: /jobs/*
Disallow: /in/*/connections
Disallow: /in/*/followers
Disallow: /messaging/
```

### Explanation | è§£èª¬:

**User-agent: \*** - Default rule
- âŒ **Disallow: /** - Everything is blocked by default | ã™ã¹ã¦ãŒãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ãƒ–ãƒ­ãƒƒã‚¯

**User-agent: LinkedInBot** - LinkedIn's own crawler
- âœ… **Allow: /** - Full access for LinkedIn's bot | LinkedInè‡ªèº«ã®ãƒœãƒƒãƒˆã¯å®Œå…¨ã‚¢ã‚¯ã‚»ã‚¹

**User-agent: Googlebot** - Selective access for Google
- âœ… **Allow: /$** - Homepage allowed | ãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸è¨±å¯
- âœ… **Allow: /company/\*** - Company pages allowed | ä¼æ¥­ãƒšãƒ¼ã‚¸è¨±å¯
- âœ… **Allow: /jobs/\*** - Job listings allowed | æ±‚äººãƒšãƒ¼ã‚¸è¨±å¯
- âŒ **Disallow: /in/\*/connections** - User connections forbidden | ãƒ¦ãƒ¼ã‚¶ãƒ¼æ¥ç¶šç¦æ­¢
- âŒ **Disallow: /in/\*/followers** - Follower lists forbidden | ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼ãƒªã‚¹ãƒˆç¦æ­¢
- âŒ **Disallow: /messaging/** - Messages forbidden | ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ç¦æ­¢

### Interpretation for Scraping | ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã«ãŠã‘ã‚‹è§£é‡ˆ:

- âŒ **General scraping:** Effectively prohibited for most bots | ã»ã¨ã‚“ã©ã®ãƒœãƒƒãƒˆã«ã¯äº‹å®Ÿä¸Šç¦æ­¢
- âš ï¸ **Legal risk:** LinkedIn actively pursues legal action against scrapers
- ğŸ’¡ **Alternative:** Use LinkedIn's official API instead
- ğŸš« **Recommendation:** Do NOT scrape LinkedIn without permission

---

## Example 4: Very Permissive robots.txt
## ä¾‹4: éå¸¸ã«å¯›å®¹ãªrobots.txt

### robots.txt Content:
```
User-agent: *
Allow: /
Crawl-delay: 1
```

### Explanation | è§£èª¬:

- âœ… **Allow: /** - Everything is allowed | ã™ã¹ã¦è¨±å¯
- â±ï¸ **Crawl-delay: 1** - Minimal 1-second delay | æœ€å°é™ã®1ç§’é…å»¶

### Interpretation | è§£é‡ˆ:

This is an **open website** that welcomes crawlers and scrapers, but still requests a polite 1-second delay to avoid overloading the server.

ã“ã‚Œã¯ã€ã‚¯ãƒ­ãƒ¼ãƒ©ãƒ¼ã‚„ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã‚’æ­“è¿ã™ã‚‹**ã‚ªãƒ¼ãƒ—ãƒ³ãªWebã‚µã‚¤ãƒˆ**ã§ã™ãŒã€ã‚µãƒ¼ãƒãƒ¼ã®éè² è·ã‚’é¿ã‘ã‚‹ãŸã‚ã€1ç§’ã®é…å»¶ã‚’è¦æ±‚ã—ã¦ã„ã¾ã™ã€‚

**Examples of such sites:**
- Open data portals (e.g., data.gov)
- Public archives
- Open source documentation

**ã“ã®ã‚ˆã†ãªã‚µã‚¤ãƒˆã®ä¾‹:**
- ã‚ªãƒ¼ãƒ—ãƒ³ãƒ‡ãƒ¼ã‚¿ãƒãƒ¼ã‚¿ãƒ«ï¼ˆä¾‹ï¼šdata.govï¼‰
- å…¬é–‹ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–
- ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ

---

## Example 5: Very Restrictive robots.txt
## ä¾‹5: éå¸¸ã«åˆ¶é™çš„ãªrobots.txt

### robots.txt Content:
```
User-agent: *
Disallow: /

User-agent: Googlebot
Disallow: /

User-agent: bingbot
Disallow: /

Sitemap: https://www.example.com/sitemap.xml
```

### Explanation | è§£èª¬:

- âŒ **Disallow: /** for all bots - Complete blocking | ã™ã¹ã¦ã®ãƒœãƒƒãƒˆã§å®Œå…¨ãƒ–ãƒ­ãƒƒã‚¯
- ğŸ—ºï¸ **Sitemap provided** - But access is denied | ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã¯æä¾›ã•ã‚Œã¦ã„ã‚‹ãŒã‚¢ã‚¯ã‚»ã‚¹ã¯æ‹’å¦

### Interpretation | è§£é‡ˆ:

This website **completely prohibits all crawling and scraping**. Even major search engines like Google and Bing are blocked.

ã“ã®Webã‚µã‚¤ãƒˆã¯ã€**ã™ã¹ã¦ã®ã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°ã¨ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’å®Œå…¨ã«ç¦æ­¢**ã—ã¦ã„ã¾ã™ã€‚Googleã‚„Bingãªã©ã®ä¸»è¦ãªæ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ã‚‚ãƒ–ãƒ­ãƒƒã‚¯ã•ã‚Œã¦ã„ã¾ã™ã€‚

**Reasons for such restrictions:**
- Private internal applications
- Membership-only content
- Legal or compliance requirements
- High security environments

**ã“ã®ã‚ˆã†ãªåˆ¶é™ã®ç†ç”±:**
- ãƒ—ãƒ©ã‚¤ãƒ™ãƒ¼ãƒˆãªå†…éƒ¨ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
- ãƒ¡ãƒ³ãƒãƒ¼å°‚ç”¨ã‚³ãƒ³ãƒ†ãƒ³ãƒ„
- æ³•çš„ã¾ãŸã¯ã‚³ãƒ³ãƒ—ãƒ©ã‚¤ã‚¢ãƒ³ã‚¹è¦ä»¶
- é«˜ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ç’°å¢ƒ

---

## Example 6: robots.txt with Crawl-delay Directive
## ä¾‹6: Crawl-delayæŒ‡ä»¤ä»˜ãrobots.txt

### robots.txt Content:
```
User-agent: *
Crawl-delay: 30
Disallow: /admin/
Disallow: /user/
Allow: /products/
Allow: /blog/

User-agent: Googlebot
Crawl-delay: 10

User-agent: BadBot
Disallow: /

Sitemap: https://www.example.com/sitemap.xml
Sitemap: https://www.example.com/blog-sitemap.xml
```

### Explanation | è§£èª¬:

**User-agent: \*** - General bots
- â±ï¸ **Crawl-delay: 30** - Wait 30 seconds between requests | 30ç§’é–“éš”ã§ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
- âŒ **Disallow: /admin/** - Admin area blocked | ç®¡ç†ã‚¨ãƒªã‚¢ãƒ–ãƒ­ãƒƒã‚¯
- âŒ **Disallow: /user/** - User profiles blocked | ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«ãƒ–ãƒ­ãƒƒã‚¯
- âœ… **Allow: /products/** - Product pages allowed | è£½å“ãƒšãƒ¼ã‚¸è¨±å¯
- âœ… **Allow: /blog/** - Blog posts allowed | ãƒ–ãƒ­ã‚°æŠ•ç¨¿è¨±å¯

**User-agent: Googlebot**
- â±ï¸ **Crawl-delay: 10** - Faster crawling allowed for Google (10 sec) | Googleã«ã¯é«˜é€Ÿã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°è¨±å¯ï¼ˆ10ç§’ï¼‰

**User-agent: BadBot**
- âŒ **Disallow: /** - Specific bot completely blocked | ç‰¹å®šã®ãƒœãƒƒãƒˆã‚’å®Œå…¨ãƒ–ãƒ­ãƒƒã‚¯

### Interpretation | è§£é‡ˆ:

This demonstrates **differential treatment** of bots:
- Most bots: 30-second delay, limited access
- Google: 10-second delay, same access rules
- Known bad bots: Completely blocked

ã“ã‚Œã¯ã€ãƒœãƒƒãƒˆã®**å·®åˆ¥çš„æ‰±ã„**ã‚’ç¤ºã—ã¦ã„ã¾ã™:
- ã»ã¨ã‚“ã©ã®ãƒœãƒƒãƒˆ: 30ç§’é…å»¶ã€é™å®šçš„ã‚¢ã‚¯ã‚»ã‚¹
- Google: 10ç§’é…å»¶ã€åŒã˜ã‚¢ã‚¯ã‚»ã‚¹ãƒ«ãƒ¼ãƒ«
- æ—¢çŸ¥ã®æ‚ªè³ªãƒœãƒƒãƒˆ: å®Œå…¨ãƒ–ãƒ­ãƒƒã‚¯

---

## Key Directives Summary | ä¸»è¦ãªæŒ‡ä»¤ã®ã¾ã¨ã‚

| Directive | Meaning | Example |
|-----------|---------|---------|
| **User-agent** | Specifies which bot the rules apply to | `User-agent: Googlebot` |
| **Disallow** | Path that bots must NOT access | `Disallow: /admin/` |
| **Allow** | Path that bots CAN access (overrides Disallow) | `Allow: /public/` |
| **Crawl-delay** | Seconds to wait between requests | `Crawl-delay: 10` |
| **Sitemap** | Location of the XML sitemap | `Sitemap: https://site.com/sitemap.xml` |
| **Request-rate** | Max requests per time period | `Request-rate: 1/10` (1 req per 10 sec) |

| æŒ‡ä»¤ | æ„å‘³ | ä¾‹ |
|------|------|-----|
| **User-agent** | ãƒ«ãƒ¼ãƒ«ãŒé©ç”¨ã•ã‚Œã‚‹ãƒœãƒƒãƒˆã‚’æŒ‡å®š | `User-agent: Googlebot` |
| **Disallow** | ãƒœãƒƒãƒˆãŒã‚¢ã‚¯ã‚»ã‚¹ã—ã¦ã¯ã„ã‘ãªã„ãƒ‘ã‚¹ | `Disallow: /admin/` |
| **Allow** | ãƒœãƒƒãƒˆãŒã‚¢ã‚¯ã‚»ã‚¹ã§ãã‚‹ãƒ‘ã‚¹ï¼ˆDisallowã‚’ä¸Šæ›¸ãï¼‰ | `Allow: /public/` |
| **Crawl-delay** | ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“ã®å¾…æ©Ÿç§’æ•° | `Crawl-delay: 10` |
| **Sitemap** | XMLã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã®å ´æ‰€ | `Sitemap: https://site.com/sitemap.xml` |
| **Request-rate** | æ™‚é–“ã‚ãŸã‚Šã®æœ€å¤§ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•° | `Request-rate: 1/10`ï¼ˆ10ç§’ã”ã¨ã«1ãƒªã‚¯ã‚¨ã‚¹ãƒˆï¼‰ |

---

## Best Practices for Web Scraping | Webã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

### âœ… DO (ã™ã¹ãã“ã¨):

1. **Always check robots.txt first** | **å¿…ãšæœ€åˆã«robots.txtã‚’ç¢ºèª**
2. **Respect crawl-delay directives** | **crawl-delayæŒ‡ä»¤ã‚’å°Šé‡**
3. **Identify your bot** with a proper User-Agent | **é©åˆ‡ãªUser-Agentã§ãƒœãƒƒãƒˆã‚’è­˜åˆ¥**
4. **Cache data** to minimize requests | **ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’æœ€å°åŒ–ã™ã‚‹ãŸã‚ã«ãƒ‡ãƒ¼ã‚¿ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥**
5. **Scrape during off-peak hours** | **ã‚ªãƒ•ãƒ”ãƒ¼ã‚¯æ™‚ã«ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°**
6. **Use official APIs** when available | **åˆ©ç”¨å¯èƒ½ãªå ´åˆã¯å…¬å¼APIã‚’ä½¿ç”¨**

### âŒ DON'T (ã—ã¦ã¯ã„ã‘ãªã„ã“ã¨):

1. **Don't ignore robots.txt** | **robots.txtã‚’ç„¡è¦–ã—ãªã„**
2. **Don't overwhelm servers** with rapid requests | **æ€¥é€Ÿãªãƒªã‚¯ã‚¨ã‚¹ãƒˆã§ã‚µãƒ¼ãƒãƒ¼ã‚’åœ§å€’ã—ãªã„**
3. **Don't scrape personal/private data** | **å€‹äºº/ãƒ—ãƒ©ã‚¤ãƒ™ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã—ãªã„**
4. **Don't use fake User-Agents** to bypass restrictions | **åˆ¶é™ã‚’å›é¿ã™ã‚‹ãŸã‚ã«å½ã®User-Agentã‚’ä½¿ç”¨ã—ãªã„**
5. **Don't scrape if explicitly prohibited** | **æ˜ç¤ºçš„ã«ç¦æ­¢ã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã—ãªã„**
6. **Don't violate Terms of Service** | **åˆ©ç”¨è¦ç´„ã«é•åã—ãªã„**

---

## Python Code: Checking robots.txt | Pythonã‚³ãƒ¼ãƒ‰: robots.txtã®ç¢ºèª

```python
from urllib.robotparser import RobotFileParser
from urllib.parse import urljoin

def can_fetch(url, user_agent='*'):
    """
    Check if a URL can be scraped according to robots.txt
    robots.txtã«å¾“ã£ã¦URLã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã§ãã‚‹ã‹ãƒã‚§ãƒƒã‚¯
    """
    # Parse the base URL
    from urllib.parse import urlparse
    parsed = urlparse(url)
    base_url = f"{parsed.scheme}://{parsed.netloc}"
    robots_url = urljoin(base_url, '/robots.txt')

    # Create parser
    rp = RobotFileParser()
    rp.set_url(robots_url)
    rp.read()

    # Check if allowed
    return rp.can_fetch(user_agent, url)

# Example usage
url = "https://www.mynavi.jp/company/12345"
if can_fetch(url):
    print(f"âœ… Scraping allowed for: {url}")
else:
    print(f"âŒ Scraping NOT allowed for: {url}")
```

---

## Legal and Ethical Considerations | æ³•çš„ãƒ»å€«ç†çš„è€ƒæ…®äº‹é …

### Important Reminders:

1. **robots.txt is NOT legally binding**, but violating it may:
   - Lead to IP blocking
   - Violate Terms of Service (which IS legally binding)
   - Result in legal action in some jurisdictions

2. **In Japan**, web scraping exists in a legal gray area. Be cautious with:
   - Personal information (å€‹äººæƒ…å ±ä¿è­·æ³• - Personal Information Protection Law)
   - Copyright-protected content (è‘—ä½œæ¨©æ³• - Copyright Law)
   - Computer fraud (ä¸æ­£ã‚¢ã‚¯ã‚»ã‚¹ç¦æ­¢æ³• - Unauthorized Access Prevention Law)

3. **Always prefer official APIs** over scraping

4. **When in doubt, seek permission** from the website owner

### é‡è¦ãªæ³¨æ„äº‹é …:

1. **robots.txtã¯æ³•çš„æ‹˜æŸåŠ›ã¯ã‚ã‚Šã¾ã›ã‚“**ãŒã€é•åã™ã‚‹ã¨:
   - IPãƒ–ãƒ­ãƒƒã‚¯ã«ã¤ãªãŒã‚‹
   - åˆ©ç”¨è¦ç´„é•åï¼ˆã“ã‚Œã¯æ³•çš„æ‹˜æŸåŠ›ã‚ã‚Šï¼‰
   - ä¸€éƒ¨ã®ç®¡è½„åŒºåŸŸã§æ³•çš„æªç½®ã«ã¤ãªãŒã‚‹

2. **æ—¥æœ¬ã§ã¯**ã€Webã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã¯æ³•çš„ã‚°ãƒ¬ãƒ¼ã‚¾ãƒ¼ãƒ³ã§ã™ã€‚ä»¥ä¸‹ã«æ³¨æ„:
   - å€‹äººæƒ…å ±ï¼ˆå€‹äººæƒ…å ±ä¿è­·æ³•ï¼‰
   - è‘—ä½œæ¨©ä¿è­·ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ï¼ˆè‘—ä½œæ¨©æ³•ï¼‰
   - ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿è©æ¬ºï¼ˆä¸æ­£ã‚¢ã‚¯ã‚»ã‚¹ç¦æ­¢æ³•ï¼‰

3. **å¸¸ã«å…¬å¼APIã‚’å„ªå…ˆ**

4. **ç–‘å•ãŒã‚ã‚‹å ´åˆã¯ã€Webã‚µã‚¤ãƒˆæ‰€æœ‰è€…ã®è¨±å¯ã‚’æ±‚ã‚ã‚‹**

---

## Exercise Questions | æ¼”ç¿’å•é¡Œ

1. What does `Disallow: /` mean? | `Disallow: /`ã¯ä½•ã‚’æ„å‘³ã—ã¾ã™ã‹ï¼Ÿ

2. If `Crawl-delay: 20` is specified, how long should you wait between requests? | `Crawl-delay: 20`ãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã€ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“ã§ã©ã®ãã‚‰ã„å¾…ã¤ã¹ãã§ã™ã‹ï¼Ÿ

3. Can you scrape a website if robots.txt blocks all bots? Why or why not? | robots.txtãŒã™ã¹ã¦ã®ãƒœãƒƒãƒˆã‚’ãƒ–ãƒ­ãƒƒã‚¯ã—ã¦ã„ã‚‹å ´åˆã€Webã‚µã‚¤ãƒˆã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã§ãã¾ã™ã‹ï¼Ÿãã®ç†ç”±ã¯ï¼Ÿ

4. What is the purpose of the `User-agent` directive? | `User-agent`æŒ‡ä»¤ã®ç›®çš„ã¯ä½•ã§ã™ã‹ï¼Ÿ

5. Write Python code to check if `https://example.com/jobs/` can be scraped. | `https://example.com/jobs/`ãŒã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯ã™ã‚‹Pythonã‚³ãƒ¼ãƒ‰ã‚’æ›¸ã„ã¦ãã ã•ã„ã€‚

---

**End of Document | è³‡æ–™çµ‚ã‚ã‚Š**
**Week 4 Training Material - ML-101 Course**
**Created for educational purposes | æ•™è‚²ç›®çš„ã§ä½œæˆ**
